% Encoding: UTF-8

@article{Badha15,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{Abdul_20,
  title={COGAM: measuring and moderating cognitive load in machine learning model explanations},
  author={Abdul, Ashraf and Von Der Weth, Christian and Kankanhalli, Mohan and Lim, Brian Y},
  booktitle={Proceedings of the 2020 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2020}
}


@article{Cardo_24,
  title={Can AI be your teammate or friend? Frequent AI users are more likely to grant humanlike roles to AI},
  author={Cardon, Peter W and Marshall, Bryan},
  journal={Business and Professional Communication Quarterly},
  volume={87},
  number={4},
  pages={654--669},
  year={2024},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}


@article{Seebe_20,
  title={Machines as teammates: A research agenda on AI in team collaboration},
  author={Seeber, Isabella and Bittner, Eva and Briggs, Robert O and De Vreede, Triparna and De Vreede, Gert-Jan and Elkins, Aaron and Maier, Ronald and Merz, Alexander B and Oeste-Rei{\ss}, Sarah and Randrup, Nils and others},
  journal={Information \& management},
  volume={57},
  number={2},
  pages={103174},
  year={2020},
  publisher={Elsevier}
}


@article{Glikso_20,
  title={Human trust in artificial intelligence: Review of empirical research},
  author={Glikson, Ella and Woolley, Anita Williams},
  journal={Academy of management annals},
  volume={14},
  number={2},
  pages={627--660},
  year={2020},
  publisher={Briarcliff Manor, NY}
}


@article{Okamu_20,
  title={Adaptive trust calibration for human-AI collaboration},
  author={Okamura, Kazuo and Yamada, Seiji},
  journal={Plos one},
  volume={15},
  number={2},
  pages={e0229132},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}


@article{Hanco_23,
  title={How and why humans trust: A meta-analysis and elaborated model},
  author={Hancock, PA and Kessler, Theresa T and Kaplan, Alexandra D and Stowers, Kimberly and Brill, J Christopher and Billings, Deborah R and Schaefer, Kristin E and Szalma, James L},
  journal={Frontiers in psychology},
  volume={14},
  pages={1081086},
  year={2023},
  publisher={Frontiers Media SA}
}


@inproceedings{Booch_21,
  title={Thinking fast and slow in AI},
  author={Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={17},
  pages={15042--15046},
  year={2021}
}


@Article{Berth_21,
  author    = {Berthet, Vincent},
  journal   = {Frontiers in psychology},
  title     = {The measurement of individual differences in cognitive biases: A review and improvement},
  year      = {2021},
  pages     = {630177},
  volume    = {12},
  publisher = {Frontiers Media SA},
}

@article{Aczel_15,
  title={Measuring individual differences in decision biases: Methodological considerations},
  author={Aczel, Balazs and Bago, Bence and Szollosi, Aba and Foldes, Andrei and Lukacs, Bence},
  journal={Frontiers in psychology},
  volume={6},
  pages={1770},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{Rong_24,
  title={Towards human-centered explainable ai: A survey of user studies for model explanations},
  author={Rong, Yao and Leemann, Tobias and Nguyen, Thai-Trang and Fiedler, Lisa and Qian, Peizhu and Unhelkar, Vaibhav and Seidel, Tina and Kasneci, Gjergji and Kasneci, Enkelejda},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={46},
  number={4},
  pages={2104--2122},
  year={2024},
  publisher={IEEE}
}


@article{Duan_24,
  title={Understanding the evolvement of trust over time within Human-AI teams},
  author={Duan, Wen and Zhou, Shiwen and Scalia, Matthew J and Yin, Xiaoyun and Weng, Nan and Zhang, Ruihao and Freeman, Guo and McNeese, Nathan and Gorman, Jamie and Tolston, Michael},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={8},
  number={CSCW2},
  pages={1--31},
  year={2024},
  publisher={ACM New York, NY, USA}
}


@article{Rotte_67,
  title={A new scale for the measurement of interpersonal trust.},
  author={Rotter, Julian B},
  journal={Journal of personality},
  year={1967},
  publisher={Blackwell Publishing}
}

@inproceedings{Bai_24,
  title={Effects of Explanations by Robots on Trust Repair in Human-Robot Collaborations},
  author={Bai, Zhangyunfan and Chen, Ke},
  booktitle={International Conference on Human-Computer Interaction},
  pages={3--14},
  year={2024},
  organization={Springer}
}

@inproceedings{Hald_21,
  title={“An error occurred!”-trust repair with virtual robot using levels of mistake explanation},
  author={Hald, Kasper and Weitz, Katharina and Andr{\'e}, Elisabeth and Rehm, Matthias},
  booktitle={Proceedings of the 9th International Conference on Human-Agent Interaction},
  pages={218--226},
  year={2021}
}

@article{Paas_94,
  title={Measurement of cognitive load in instructional research},
  author={Paas, Fred GWC and Van Merri{\"e}nboer, Jeroen JG and Adam, Jos J},
  journal={Perceptual and motor skills},
  volume={79},
  number={1},
  pages={419--430},
  year={1994},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{Beatt_00,
  title={The pupillary system.},
  author={Beatty, Jackson and Lucero-Wagoner, Brennis},
  year={2000},
  publisher={Cambridge University Press}
}

@article{Badde_92,
  title={Working memory: The interface between memory and cognition},
  author={Baddeley, Alan},
  journal={Journal of cognitive neuroscience},
  volume={4},
  number={3},
  pages={281--288},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@incollection{Hart_88,
  title={Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research},
  author={Hart, Sandra G and Staveland, Lowell E},
  booktitle={Advances in psychology},
  volume={52},
  pages={139--183},
  year={1988},
  publisher={Elsevier}
}

@article{Swell_10,
  title={Element interactivity and intrinsic, extraneous, and germane cognitive load},
  author={Sweller, John},
  journal={Educational psychology review},
  volume={22},
  pages={123--138},
  year={2010},
  publisher={Springer}
}


@article{Swell_98,
  title={Cognitive architecture and instructional design},
  author={Sweller, John and Van Merrienboer, Jeroen JG and Paas, Fred GWC},
  journal={Educational psychology review},
  volume={10},
  pages={251--296},
  year={1998},
  publisher={Springer}
}


@article{Glock_11,
  title={An eye-tracking study on information processing in risky decisions: Evidence for compensatory strategies based on automatic processes},
  author={Gl{\"o}ckner, Andreas and Herbold, Ann-Katrin},
  journal={Journal of Behavioral Decision Making},
  volume={24},
  number={1},
  pages={71--98},
  year={2011},
  publisher={Wiley Online Library}
}

@article{Whitn_08,
  title={Framing effects under cognitive load: The role of working memory in risky decisions},
  author={Whitney, Paul and Rinehart, Christa A and Hinson, John M},
  journal={Psychonomic bulletin \& review},
  volume={15},
  pages={1179--1184},
  year={2008},
  publisher={Springer}
}

@article{Colqu_7,
  title={Trust, trustworthiness, and trust propensity: a meta-analytic test of their unique relationships with risk taking and job performance.},
  author={Colquitt, Jason A and Scott, Brent A and LePine, Jeffery A},
  journal={Journal of applied psychology},
  volume={92},
  number={4},
  pages={909},
  year={2007},
  publisher={American Psychological Association}
}

@article{Raiaa_24,
  title={A review on large language models: Architectures, applications, taxonomies, open issues and challenges},
  author={Raiaan, Mohaimenul Azam Khan and Mukta, Md Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
  journal={IEEE access},
  volume={12},
  pages={26839--26874},
  year={2024},
  publisher={IEEE}
}

@inproceedings{Nikol_24,
  title={The Experimental Evaluation of Different Explainable AI Techniques for Large Language Models},
  author={Nikoli{\'c}, Mina and Stanimirovi{\'c}, Aleksandar and Stojkovi{\'c}, Suzana},
  booktitle={Conference on Information Technology and its Applications},
  pages={299--310},
  year={2024},
  organization={Springer}
}

@inproceedings{Kraus_23,
  title={Commonsense reasoning and explainable artificial intelligence using large language models},
  author={Krause, Stefanie and Stolzenburg, Frieder},
  booktitle={European Conference on Artificial Intelligence},
  pages={302--319},
  year={2023},
  organization={Springer}
}


@article{Ha_24,
  title={Improving trust in AI with mitigating confirmation bias: Effects of explanation type and debiasing strategy for decision-making with explainable AI},
  author={Ha, Taehyun and Kim, Sangyeon},
  journal={International journal of human--computer interaction},
  volume={40},
  number={24},
  pages={8562--8573},
  year={2024},
  publisher={Taylor \& Francis}
}


@inproceedings{Noura_21,
  title={Anchoring bias affects mental model formation and user reliance in explainable AI systems},
  author={Nourani, Mahsan and Roy, Chiradeep and Block, Jeremy E and Honeycutt, Donald R and Rahman, Tahrima and Ragan, Eric and Gogate, Vibhav},
  booktitle={Proceedings of the 26th International Conference on Intelligent User Interfaces},
  pages={340--350},
  year={2021}
}

@article{Stepi_21,
  title={A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence},
  author={Stepin, Ilia and Alonso, Jose M and Catala, Alejandro and Pereira-Fari{\~n}a, Mart{\'\i}n},
  journal={Ieee Access},
  volume={9},
  pages={11974--12001},
  year={2021},
  publisher={IEEE}
}

@article{Hasti_17,
  title={Generalized additive models},
  author={Hastie, Trevor J},
  journal={Statistical models in S},
  pages={249--307},
  year={2017},
  publisher={Routledge}
}


@article{Fishe_19,
  title={All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously},
  author={Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={177},
  pages={1--81},
  year={2019}
}

@misc{Hasti_09,
  title={The elements of statistical learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{Selva_17,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{Neder_85,
  title={Methods of coping with social desirability bias: A review},
  author={Nederhof, Anton J},
  journal={European journal of social psychology},
  volume={15},
  number={3},
  pages={263--280},
  year={1985},
  publisher={Wiley Online Library}
}


@misc{Ullma_19,
  title={MDMT: Multi-dimensional measure of trust},
  author={Ullman, Daniel and Malle, Bertram F},
  year={2019},
  publisher={New York: IEEE}
}

@article{Dael_12,
  title={Emotion expression in body action and posture.},
  author={Dael, Nele and Mortillaro, Marcello and Scherer, Klaus R},
  journal={Emotion},
  volume={12},
  number={5},
  pages={1085},
  year={2012},
  publisher={American Psychological Association}
}


@article{Nagen_24,
  title={Eye tracking insights into physician behaviour with safe and unsafe explainable AI recommendations},
  author={Nagendran, Myura and Festor, Paul and Komorowski, Matthieu and Gordon, Anthony C and Faisal, Aldo A},
  journal={NPJ Digital Medicine},
  volume={7},
  number={1},
  pages={202},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{Henri_13,
  title={Accessing emotion patterns from affective interactions using electrodermal activity},
  author={Henriques, Rui and Paiva, Ana and Antunes, Claudia},
  booktitle={2013 humaine association conference on affective computing and intelligent interaction},
  pages={43--48},
  year={2013},
  organization={IEEE}
}

@article{Chura_22,
  title={Domain-incremental continual learning for mitigating bias in facial expression and action unit recognition},
  author={Churamani, Nikhil and Kara, Ozgur and Gunes, Hatice},
  journal={IEEE Transactions on Affective Computing},
  volume={14},
  number={4},
  pages={3191--3206},
  year={2022},
  publisher={IEEE}
}


@article{Kelly_23,
  title={What factors contribute to the acceptance of artificial intelligence? A systematic review},
  author={Kelly, Sage and Kaye, Sherrie-Anne and Oviedo-Trespalacios, Oscar},
  journal={Telematics and Informatics},
  volume={77},
  pages={101925},
  year={2023},
  publisher={Elsevier}
}


@inproceedings{Rahma_24,
  title={Motivation, Concerns, and Attitudes Towards AI: Differences by Gender, Age, and Culture},
  author={Rahman, Mohammad Mominur and Babiker, Areej and Ali, Raian},
  booktitle={International Conference on Web Information Systems Engineering},
  pages={375--391},
  year={2024},
  organization={Springer}
}


@inproceedings{Armut_24,
  title={Artificial Intelligence--Gender-Specific Differences in Perception, Understanding, and Training Interest},
  author={Armutat, Sascha and Wattenberg, Malte and Mauritz, Nina},
  booktitle={International Conference on Gender Research},
  volume={7},
  number={1},
  pages={36--43},
  year={2024}
}


@article{Ahn_22,
  title={The effect of gender stereotypes on artificial intelligence recommendations},
  author={Ahn, Jungyong and Kim, Jungwon and Sung, Yongjun},
  journal={Journal of Business Research},
  volume={141},
  pages={50--59},
  year={2022},
comment = {gender stereotypes in e-commerce},
  publisher={Elsevier}
}


@inproceedings{Madse_00,
  title={Measuring human-computer trust},
  author={Madsen, Maria and Gregor, Shirley},
  booktitle={11th australasian conference on information systems},
  volume={53},
  pages={6--8},
  year={2000},
  organization={Citeseer}
}


@article{Kahne_11,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  journal={Farrar, Straus and Giroux},
  year={2011}
}


@article{Venka_03,
  title={User acceptance of information technology: Toward a unified view},
  author={Venkatesh, Viswanath and Morris, Michael G and Davis, Gordon B and Davis, Fred D},
  journal={MIS quarterly},
  pages={425--478},
  year={2003},
  publisher={JSTOR}
}

@article{Davis_89,
  title={User acceptance of computer technology: A comparison of two theoretical models},
  author={Davis, Fred D and Bagozzi, Richard P and Warshaw, Paul R},
  journal={Management science},
  volume={35},
  number={8},
  pages={982--1003},
  year={1989},
  publisher={INFORMS}
}

@article{Diebe_22,
  title={A novel model usability evaluation framework (MUsE) for explainable artificial intelligence},
  author={Dieber, J{\"u}rgen and Kirrane, Sabrina},
  journal={Information Fusion},
  volume={81},
  pages={143--153},
  year={2022},
  comment = {check to include to the studies},
  publisher={Elsevier}
}


@article{Dietv_18,
  title={Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them},
  author={Dietvorst, Berkeley J and Simmons, Joseph P and Massey, Cade},
  journal={Management science},
  volume={64},
  number={3},
  pages={1155--1170},
  year={2018},
  publisher={INFORMS}
}


@article{Riege_24,
  title={The (Im) perfect automation schema: Who is trusted more, automated or human decision support?},
  author={Rieger, Tobias and Kugler, Luisa and Manzey, Dietrich and Roesler, Eileen},
  journal={Human Factors},
  volume={66},
  number={8},
  pages={1995--2007},
  year={2024},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{He_23,
  title={CETD: Counterfactual Explanations by Considering Temporal Dependencies in Sequential Recommendation},
  author={He, Ming and An, Boyang and Wang, Jiwen and Wen, Hao},
  journal={Applied Sciences},
  volume={13},
  number={20},
  pages={11176},
  year={2023},
  publisher={MDPI}
}


@article{Bach_24,
  title={A systematic literature review of user trust in AI-enabled systems: An HCI perspective},
  author={Bach, Tita Alissa and Khan, Amna and Hallock, Harry and Beltr{\~a}o, Gabriela and Sousa, Sonia},
  journal={International Journal of Human--Computer Interaction},
  volume={40},
  number={5},
  pages={1251--1266},
  year={2024},
  publisher={Taylor \& Francis}
}



@article{Mayer_95,
  title={An Integrative Model of Organizational Trust},
  author={Mayer, RC},
  journal={Academy of Management Review},
  year={1995}
}


@book{Hidal_21,
  title={How humans judge machines},
  author={Hidalgo, C{\'e}sar A and Orghian, Diana and Canals, Jordi Albo and De Almeida, Filipa and Martin, Natalia},
  year={2021},
  publisher={MIT Press}
}


@article{Hasel_15,
  title={The evolution of cognitive bias},
  author={Haselton, Martie G and Nettle, Daniel and Andrews, Paul W},
  journal={The handbook of evolutionary psychology},
  pages={724--746},
  year={2015},
  publisher={Wiley Online Library}
}


@article{Samhi_13,
  title={The “clever Hans phenomenon” revisited},
  author={Samhita, Laasya and Gross, Hans J},
  journal={Communicative \& integrative biology},
  volume={6},
  number={6},
  pages={e27122},
  year={2013},
  publisher={Taylor \& Francis}
}


@article{Rasmu_82,
  title={Human errors. A taxonomy for describing human malfunction in industrial installations},
  author={Rasmussen, Jens},
  journal={Journal of occupational accidents},
  volume={4},
  number={2-4},
  pages={311--333},
  year={1982},
  publisher={Elsevier}
}


@article{Dacey_17,
  title={Anthropomorphism as cognitive bias},
  author={Dacey, Mike},
  journal={Philosophy of Science},
  volume={84},
  number={5},
  pages={1152--1164},
  year={2017},
  publisher={Cambridge University Press}
}


@article{Mille_17,
  title={Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences},
  author={Miller, Tim and Howe, Piers and Sonenberg, Liz},
  journal={arXiv preprint arXiv:1712.00547},
  year={2017}
}


@article{Wells_21,
  title={Explainable ai and reinforcement learning—a systematic review of current approaches and trends},
  author={Wells, Lindsay and Bednarz, Tomasz},
  journal={Frontiers in artificial intelligence},
  volume={4},
  pages={550030},
  year={2021},
  publisher={Frontiers Media SA}
}


@Article{Merwe_24,
  author    = {van de Merwe, Koen and Mallam, Steven and Nazir, Salman},
  journal   = {Human Factors},
  title     = {Agent transparency, situation awareness, mental workload, and operator performance: A systematic literature review},
  year      = {2024},
  number    = {1},
  pages     = {180--208},
  volume    = {66},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}


@book{Broni_21,
  title={Psychological foundations of explainability and interpretability in artificial intelligence},
  author={Broniatowski, David A and Broniatowski, David A},
  year={2021},
  publisher={US Department of Commerce, National Institute of Standards and Technology}
}

@article{Franc_18,
  title={An introduction to deep reinforcement learning},
  author={Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={3-4},
  pages={219--354},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@article{Bomma_21,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{Vaswa_17,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{LeCun_95,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995},
  publisher={Citeseer}
}

@article{Gerne_23,
  title={How virtual reality is used in industrial maritime design processes: Two case studies},
  author={Gernez, Etienne and Nordby, Kjetil and Dreyer, Simon Archer and Bur{\aa}s, Theodor and Fauske, Jon Erling},
  journal={Ocean Engineering},
  volume={283},
  pages={115091},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{Houwe_23,
  title={An Approach for Measuring The Effects of Augmented Reality on Human Performance in Maritime Navigation},
  author={Houweling, Koen Pieter and Mallam, Steven C and van de Merwe, Koen and Nordby, Kjetil},
  booktitle={Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume={67},
  number={1},
  pages={1591--1597},
  year={2023},
  organization={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{Selva_17,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}


@InProceedings{Jirak_18,
  author       = {Jirak, Doreen and Wermter, Stefan},
  booktitle    = {2018 International Joint Conference on Neural Networks (IJCNN)},
  title        = {Sparse autoencoders for posture recognition},
  year         = {2018},
  organization = {IEEE},
  pages        = {1--10},
}

@InProceedings{Ribei_16,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  year      = {2016},
  pages     = {1135--1144},
}

@InProceedings{Molnar_20,
  author       = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
  booktitle    = {Joint European conference on machine learning and knowledge discovery in databases},
  title        = {Interpretable machine learning--a brief history, state-of-the-art and challenges},
  year         = {2020},
  organization = {Springer},
  pages        = {417--431},
}

@inproceedings{Sunda_20,
  title={The many Shapley values for model explanation},
  author={Sundararajan, Mukund and Najmi, Amir},
  booktitle={International conference on machine learning},
  pages={9269--9278},
  year={2020},
  organization={PMLR}
}

@Article{Zhou_24,
  author    = {Zhou, Ziyuan and Liu, Guanjun and Tang, Ying},
  journal   = {IEEE Transactions on Intelligent Vehicles},
  title     = {Multiagent Reinforcement Learning: Methods, Trustworthiness, Applications in Intelligent Vehicles, and Challenges},
  year      = {2024},
  comment   = {related work},
  publisher = {IEEE},
}


@InProceedings{Zhang_21,
  author    = {Zhang, Zelun Tony and Liu, Yuanting and Hußmann, Heinrich},
  booktitle = {2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS)},
  title     = {Pilot Attitudes Toward AI in the Cockpit: Implications for Design},
  year      = {2021},
  pages     = {1-6},
  comment   = {study},
  doi       = {10.1109/ICHMS53169.2021.9582448},
  keywords  = {Industries;Conferences;Complexity theory;Stakeholders;Artificial intelligence;Usability;Interviews;interviews;thematic analysis;intelligent cockpit assistant systems;human-AI interaction;imperfect AI},
}


@Article{Xu_23,
  author    = {Xu, Wei and Dainoff, Marvin J and Ge, Liezhong and Gao, Zaifeng},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Transitioning to human interaction with AI systems: New challenges and opportunities for HCI professionals to enable human-centered AI},
  year      = {2023},
  number    = {3},
  pages     = {494--518},
  volume    = {39},
  comment   = {review},
  publisher = {Taylor \& Francis},
}


@Misc{Xiong_22,
  author  = {Xiong, W and Fan, H and Ma, L and Wang, C},
  title   = {Challenges of human--machine collaboration in risky decision-making. Front Eng Manag 9 (1): 89--103 (2022)},
  year    = {2022},
  comment = {review, cognitive bias},
}

@Article{Wang_24,
  author   = {Yanyun (Mia) Wang and Weizi Liu and Mike Yao},
  journal  = {New Media \& Society},
  title    = {Which recommendation system do you trust the most? Exploring the impact of perceived anthropomorphism on recommendation system trust, choice confidence, and information disclosure},
  year     = {2024},
  number   = {0},
  pages    = {14614448231223517},
  volume   = {0},
  abstract = {Recommendation systems (RSs) leverage data and algorithms to generate a set of suggestions to reduce consumers’ efforts and assist their decisions. In this study, we examine how different framings of recommendations trigger people’s anthropomorphic perceptions of RSs and therefore affect users’ attitudes in an online experiment. Participants used and evaluated one of four versions of a web-based wine RS with different source framings (i.e. “recommendation by an algorithm,” “recommendation by an AI assistant,” “recommendation by knowledge generated from similar people,” no description). Results showed that different source framings generated different levels of perceived anthropomorphism. Participants indicated greater trust in the recommendations and greater confidence in making choices based on the recommendations when they perceived an RS as highly anthropomorphic; however, higher perceived anthropomorphism of an RS led to a lower willingness to disclose personal information to the RS.},
  comment  = {related work; user preference/presentation of recommendation; role of anthropomorphism},
  doi      = {10.1177/14614448231223517},
  eprint   = {https://doi.org/10.1177/14614448231223517},
  url      = {https://doi.org/10.1177/14614448231223517},
}


@Article{Veitc_22,
  author   = {Erik Veitch and Ole {Andreas Alsos}},
  journal  = {Safety Science},
  title    = {A systematic review of human-AI interaction in autonomous ship systems},
  year     = {2022},
  issn     = {0925-7535},
  pages    = {105778},
  volume   = {152},
  abstract = {Automation is increasing in shipping. Advancements in Artificial Intelligence (AI) applications like collision avoidance and computer vision have the potential to augment or take over the roles of ship navigators. However, implementation of AI technologies may also jeopardize safety if done in a way that reduces human control. In this systematic review, we included 42 studies about human supervision and control of autonomous ships. We addressed three research questions (a) how is human control currently being adopted in autonomous ship systems? (b) what methods, approaches, and theories are being used to address safety concerns and design challenges? and (c) what research gaps, regulatory obstacles, and technical shortcomings represent the most significant barriers to their implementation? We found that (1) human operators have an active role in ensuring autonomous ship safety above and beyond a backup role, (2) System-Theoretic Process Analysis and Bayesian Networks are the most common risk assessment tools in risk-based design, and (3) the new role of shore control center operators will require new competencies and training. The field of autonomous ship research is growing quickly. New risks are emerging from increasing interaction with AI systems in safety–critical systems, underscoring new research questions. Effective human-AI interaction design is predicated on increased cross-disciplinary efforts, requiring reconciling productivity with safety (resilience), technical limitations with human abilities and expectations (interaction design), and machine task autonomy with human supervisory control (safety management).},
  comment  = {related work/review; maritime context},
  doi      = {https://doi.org/10.1016/j.ssci.2022.105778},
  keywords = {Automation, Artificial Intelligence, Work, Safety, Marine Navigation, Human-Computer Interaction, Safety management, Resilience Engineering, Interaction Design, Maritime Autonomous Surface Ships, STPA, Bayesian Networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925753522001175},
}

@Article{Rooy_23,
  author  = {Van Rooy, Dirk},
  title   = {Designing the interaction between humans and autonomous systems: The role of behavioral science.},
  year    = {2023},
  comment = {related work; interface behavioral science and HMI (HAT)},
}


@Article{Tocch_22,
  author         = {Tocchetti, Andrea and Brambilla, Marco},
  journal        = {Data},
  title          = {The Role of Human Knowledge in Explainable AI},
  year           = {2022},
  issn           = {2306-5729},
  number         = {7},
  volume         = {7},
  abstract       = {As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, i.e., high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability method is to faithfully describe the behaviour of a (black-box) model to users who can get a better understanding of its logic, thus increasing the trust and acceptance of the system. Unfortunately, state-of-the-art explainability approaches may not be enough to guarantee the full understandability of explanations from a human perspective. For this reason, human-in-the-loop methods have been widely employed to enhance and/or evaluate explanations of machine learning models. These approaches focus on collecting human knowledge that AI systems can then employ or involving humans to achieve their objectives (e.g., evaluating or improving the system). This article aims to present a literature overview on collecting and employing human knowledge to improve and evaluate the understandability of machine learning models through human-in-the-loop approaches. Furthermore, a discussion on the challenges, state-of-the-art, and future trends in explainability is also provided.},
  article-number = {93},
  comment        = {review},
  doi            = {10.3390/data7070093},
  url            = {https://www.mdpi.com/2306-5729/7/7/93},
}


@InProceedings{Teso_19,
  author    = {Teso, Stefano and Kersting, Kristian},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  title     = {Explanatory interactive machine learning},
  year      = {2019},
  pages     = {239--245},
  comment   = {related work; design of xai, interaction},
}

@Article{Sperr_21,
  author   = {Sperrle, F. and El-Assady, M. and Guo, G. and Borgo, R. and Chau, D. Horng and Endert, A. and Keim, D.},
  journal  = {Computer Graphics Forum},
  title    = {A Survey of Human-Centered Evaluations in Human-Centered Machine Learning},
  year     = {2021},
  number   = {3},
  pages    = {543-568},
  volume   = {40},
  abstract = {Abstract Visual analytics systems integrate interactive visualizations and machine learning to enable expert users to solve complex analysis tasks. Applications combine techniques from various fields of research and are consequently not trivial to evaluate. The result is a lack of structure and comparability between evaluations. In this survey, we provide a comprehensive overview of evaluations in the field of human-centered machine learning. We particularly focus on human-related factors that influence trust, interpretability, and explainability. We analyze the evaluations presented in papers from top conferences and journals in information visualization and human-computer interaction to provide a systematic review of their setup and findings. From this survey, we distill design dimensions for structured evaluations, identify evaluation gaps, and derive future research opportunities.},
  comment  = {related work},
  doi      = {https://doi.org/10.1111/cgf.14329},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14329},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14329},
}


@Article{Rohlf_21,
  author   = {Rohlfing, Katharina J. and Cimiano, Philipp and Scharlau, Ingrid and Matzner, Tobias and Buhl, Heike M. and Buschmeier, Hendrik and Esposito, Elena and Grimminger, Angela and Hammer, Barbara and Häb-Umbach, Reinhold and Horwath, Ilona and Hüllermeier, Eyke and Kern, Friederike and Kopp, Stefan and Thommes, Kirsten and Ngonga Ngomo, Axel-Cyrille and Schulte, Carsten and Wachsmuth, Henning and Wagner, Petra and Wrede, Britta},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems},
  title    = {Explanation as a Social Practice: Toward a Conceptual Framework for the Social Design of AI Systems},
  year     = {2021},
  issn     = {2379-8939},
  month    = {Sep.},
  number   = {3},
  pages    = {717-728},
  volume   = {13},
  abstract = {The recent surge of interest in explainability in artificial intelligence (XAI) is propelled by not only technological advancements in machine learning but also by regulatory initiatives to foster transparency in algorithmic decision making. In this article, we revise the current concept of explainability and identify three limitations: passive explainee, narrow view on the social process, and undifferentiated assessment of explainee’s understanding. In order to overcome these limitations, we present explanation as a social practice in which explainer and explainee co-construct understanding on the microlevel. We view the co-construction on a microlevel as embedded into a macrolevel, yielding expectations concerning, e.g., social roles or partner models: typically, the role of the explainer is to provide an explanation and to adapt it to the current level of explainee’s understanding; the explainee, in turn, is expected to provide cues that direct the explainer. Building on explanations being a social practice, we present a conceptual framework that aims to guide future research in XAI. The framework relies on the key concepts of monitoring and scaffolding to capture the development of interaction. We relate our conceptual framework and our new perspective on explaining to transparency and autonomy as objectives considered for XAI.},
  comment  = {related work},
  doi      = {10.1109/TCDS.2020.3044366},
  keywords = {Artificial intelligence;Receivers;Monitoring;Machine learning algorithms;Computational modeling;Task analysis;Surges;Explainability;explainable artificial systems;process of explaining and understanding},
}


@Misc{Rodem_24,
  author        = {Julian Rodemann and Federico Croppi and Philipp Arens and Yusuf Sale and Julia Herbinger and Bernd Bischl and Eyke Hüllermeier and Thomas Augustin and Conor J. Walsh and Giuseppe Casalicchio},
  title         = {Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {related work for xai methods},
  eprint        = {2403.04629},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2403.04629},
}


@Article{Rieth_22,
  author    = {Rieth, Mich{\`e}le and Hagemann, Vera},
  journal   = {Applied Ergonomics},
  title     = {Automation as an equal team player for humans?--A view into the field and implications for research and practice},
  year      = {2022},
  pages     = {103552},
  volume    = {98},
  comment   = {review},
  publisher = {Elsevier},
}


@Article{Raees_24,
  author   = {Muhammad Raees and Inge Meijerink and Ioanna Lykourentzou and Vassilis-Javed Khan and Konstantinos Papangelis},
  journal  = {International Journal of Human-Computer Studies},
  title    = {From explainable to interactive AI: A literature review on current trends in human-AI interaction},
  year     = {2024},
  issn     = {1071-5819},
  pages    = {103301},
  volume   = {189},
  abstract = {AI systems are increasingly being adopted across various domains and application areas. With this surge, there is a growing research focus and societal concern for actively involving humans in developing, operating, and adopting these systems. Despite this concern, most existing literature on AI and Human–Computer Interaction (HCI) primarily focuses on explaining how AI systems operate and, at times, allowing users to contest AI decisions. Existing studies often overlook more impactful forms of user interaction with AI systems, such as giving users agency beyond contestability and enabling them to adapt and even co-design the AI’s internal mechanics. In this survey, we aim to bridge this gap by reviewing the state-of-the-art in Human-Centered AI literature, the domain where AI and HCI studies converge, extending past Explainable and Contestable AI, delving into the Interactive AI and beyond. Our analysis contributes to shaping the trajectory of future Interactive AI design and advocates for a more user-centric approach that provides users with greater agency, fostering not only their understanding of AI’s workings but also their active engagement in its development and evolution.},
  comment  = {review},
  doi      = {https://doi.org/10.1016/j.ijhcs.2024.103301},
  keywords = {Human-centered AI, Interactivity, Collaboration, Explainability},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581924000855},
}


@Article{Pfeuf_23,
  author    = {Pfeuffer, Nicolas and Baum, Lorenz and Stammer, Wolfgang and Abdel-Karim, Benjamin M and Schramowski, Patrick and Bucher, Andreas M and H{\"u}gel, Christian and Rohde, Gernot and Kersting, Kristian and Hinz, Oliver},
  journal   = {Business \& Information Systems Engineering},
  title     = {Explanatory Interactive Machine Learning: Establishing an Action Design Research Process for Machine Learning Projects},
  year      = {2023},
  number    = {6},
  pages     = {677--701},
  volume    = {65},
  comment   = {related work; medical domain but focus on interactive interface; case study on lungs and xai diagnosis (images, cnn)},
  publisher = {Springer},
}


@Article{Longo_24,
  author    = {Longo, Luca and Brcic, Mario and Cabitza, Federico and Choi, Jaesik and Confalonieri, Roberto and Del Ser, Javier and Guidotti, Riccardo and Hayashi, Yoichi and Herrera, Francisco and Holzinger, Andreas and others},
  journal   = {Information Fusion},
  title     = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
  year      = {2024},
  pages     = {102301},
  volume    = {106},
  comment   = {review/related work},
  publisher = {Elsevier},
}


@InCollection{Okoro_24,
  author    = {E.M. Okoro and A.O. Umagba and B.A. Abara and Z.S. Isa and A. Buhari},
  booktitle = {XAI Based Intelligent Systems for Society 5.0},
  publisher = {Elsevier},
  title     = {Chapter 2 - Towards explainable artificial intelligence: history, present scenarios, and future trends},
  year      = {2024},
  editor    = {Fadi Al-Turjman and Anand Nayyar and Mohd Naved and Anuj Kumar Singh and Muhammad Bilal},
  isbn      = {978-0-323-95315-3},
  pages     = {29-59},
  abstract  = {As the field of artificial intelligence (AI) continues to advance, the need for explainable AI (XAI) has become increasingly evident. XAI refers to a collection of machine learning (ML) algorithms that aim to provide greater transparency and interpretability while maintaining high-performance levels. The ultimate goal of XAI is to empower individuals to comprehend, trust, and manage AI technologies. While early AI systems were relatively straightforward and easily interpretable, the advent of deep neural networks (DNNs) and other opaque decision systems has challenged the traditional notion of interpretable AI. These opaque models, while highly effective, are often met with skepticism and mistrust, as they can be perceived as “black boxes” that are difficult to understand and explain. As a result, there has been a growing interest in developing XAI techniques that can help address these concerns. In this chapter, we will delve into the history and evolution of XAI, examining the key events and milestones that have shaped the field to date. This chapter provides a systematic analysis of the history and evolution of XAI, exploring the critical events and milestones that have shaped the field to date. We offer a comprehensive review of XAI methods and approaches and discuss recent trends and perspectives that are driving the future of XAI. Additionally, we explore the benefits and drawbacks of XAI, including improved interpretability and reduced bias, as well as the challenges of ensuring the security and privacy of XAI models. Finally, we provide critical observations and conclusions that summarize the current state of XAI, its prospects, and future direction, with the aim of providing a thorough understanding of the subject.},
  comment   = {related work, xai methods},
  doi       = {https://doi.org/10.1016/B978-0-323-95315-3.00006-1},
  keywords  = {Artificial neural networks (ANN), Explainability methods, Explainable AI (XAI), ML algorithms, Model interpretability, Transparent AI},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780323953153000061},
}


@Article{Muell_24c,
  author    = {M{\"u}ller, Romy and Tho{\ss}, Marius and Ullrich, Julian and Seitz, Steffen and Knoll, Carsten},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Interpretability is in the eye of the beholder: Human versus artificial classification of image segments generated by humans versus XAI},
  year      = {2024},
  pages     = {1--23},
  comment   = {study; check if duplicate (or journal extension) with Do humans and CNNs... study 2024; related work},
  publisher = {Taylor \& Francis},
}


@Article{Muell_24b,
  author    = {M{\"u}ller, Romy and Duerschmidt, Marcel and Ullrich, Julian and Knoll, Carsten and Weber, Sascha and Seitz, Steffen},
  journal   = {Applied Sciences},
  title     = {Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type},
  year      = {2024},
  number    = {6},
  pages     = {2648},
  volume    = {14},
  comment   = {study but more on human factors when presenting XAI; related work},
  publisher = {MDPI},
}


@Article{Muell_24a,
  author  = {M{\"u}ller, Romy},
  journal = {arXiv preprint arXiv:2404.16042},
  title   = {How explainable AI affects human performance: A systematic review of the behavioural consequences of saliency maps},
  year    = {2024},
  comment = {related work/review; saliency maps, check for redundancy (1st author Romy Mueller)},
}



@Article{Media_22,
  author         = {Medianovskyi, Kyrylo and Pietarinen, Ahti-Veikko},
  journal        = {Philosophies},
  title          = {On Explainable AI and Abductive Inference},
  year           = {2022},
  issn           = {2409-9287},
  number         = {2},
  volume         = {7},
  abstract       = {Modern explainable AI (XAI) methods remain far from providing human-like answers to ‘why’ questions, let alone those that satisfactorily agree with human-level understanding. Instead, the results that such methods provide boil down to sets of causal attributions. Currently, the choice of accepted attributions rests largely, if not solely, on the explainee’s understanding of the quality of explanations. The paper argues that such decisions may be transferred from a human to an XAI agent, provided that its machine-learning (ML) algorithms perform genuinely abductive inferences. The paper outlines the key predicament in the current inductive paradigm of ML and the associated XAI techniques, and sketches the desiderata for a truly participatory, second-generation XAI, which is endowed with abduction.},
  article-number = {35},
  comment        = {related work; xai methods; outlines current xai method limitations, call for abductive models},
  doi            = {10.3390/philosophies7020035},
  url            = {https://www.mdpi.com/2409-9287/7/2/35},
}



@article{Barro_20,
  title={The facechannel: a fast and furious deep neural network for facial expression recognition},
  author={Barros, Pablo and Churamani, Nikhil and Sciutti, Alessandra},
  journal={SN Computer Science},
  volume={1},
  number={6},
  pages={321},
  year={2020},
  publisher={Springer}
}


@Article{Marti_24,
  author    = {Martini, Barbara and Bellisario, Denise and Coletti, Paola},
  journal   = {Sustainability},
  title     = {Human-Centered and Sustainable Artificial Intelligence in Industry 5.0: Challenges and Perspectives},
  year      = {2024},
  number    = {13},
  pages     = {5448},
  volume    = {16},
  comment   = {related work},
  publisher = {Multidisciplinary Digital Publishing Institute},
}

@article{Bergh_24,
  title={Human performance and automated operations: a regulatory perspective},
  author={Bergh, Linn Iren Vestly and Teigen, Kristian Solheim and D{\o}rum, Fredrik},
  journal={Ergonomics},
  volume={67},
  number={6},
  pages={744--758},
  year={2024},
  publisher={Taylor \& Francis}
}


@Article{Guill_23a,
  author    = {Le Guillou, Marin and Pr{\'e}vot, Laurent and Berberian, Bruno},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Bringing together ergonomic concepts and cognitive mechanisms for human—AI agents cooperation},
  year      = {2023},
  number    = {9},
  pages     = {1827--1840},
  volume    = {39},
  comment   = {review},
  publisher = {Taylor \& Francis},
}

@inproceedings{Guill_23b,
  title={Trusting artificial agents: Communication trumps performance},
  author={Le Guillou, Marin and Pr{\'e}vot, Laurent and Berberian, Bruno},
  booktitle={AAMAS 2023},
  year={2023}
}

@Article{Kumar_24,
  author   = {Kumar, Sushant and Datta, Sumit and Singh, Vishakha and Datta, Deepanwita and Kumar Singh, Sanjay and Sharma, Ritesh},
  journal  = {IEEE Access},
  title    = {Applications, Challenges, and Future Directions of Human-in-the-Loop Learning},
  year     = {2024},
  issn     = {2169-3536},
  pages    = {75735-75760},
  volume   = {12},
  abstract = {Machine learning (ML) has become a popular technique for various automation tasks in the era of Industry 4.0, such as the analysis and synthesis of visual data such as images and videos, natural language and speech, financial data, and biomedical applications. However, ML-based automation techniques are facing difficulties like decision-making, thus incorporating user expertise into the system might be advantageous. The goal of adding human domain expertise with ML-based automation is to provide more accurate prediction models. Human-in-the-loop (HITL) systems that integrate human expertise with ML algorithms are becoming more and more common in various industries. However, there are a number of methodological, technical, and ethical difficulties with the development and application of HITL systems. This paper aims to explore the methodologies, challenges, and opportunities associated with HITL systems implementations. We also discuss a number of issues that must be resolved for HITL systems to be effective, including data quality, bias, and user engagement. Besides, we also explored several approaches that can be utilized to enhance the performance of HITL systems, such as active learning (AL), iterative ML, and reinforcement learning, as well as the current state of the art in HITL systems. We also selectively highlighted the advantages of HITL systems, such as their potential to increase decision-making process accountability and transparency by utilizing human experience to improve ML decision-making capability. The paper will be very useful for researchers, practitioners, and policymakers.},
  comment  = {related work; HITL},
  doi      = {10.1109/ACCESS.2024.3401547},
  keywords = {Task analysis;Data models;Artificial intelligence;Annotations;Human in the loop;Training;Natural language processing;Human in the loop;Machine learning;Fourth Industrial Revolution;Human-in-the-loop (HITL);machine learning algorithms;accountability;transparency},
}


@Article{Knick_23,
  author  = {Knickrehm, Charlotte and Voss, Marleen and Barton, Marie-Christin},
  journal = {ECIS 2023 Research Papers},
  title   = {Can You Trust Me? Reviewing More Than Three Decades of AI Trust Literature},
  year    = {2023},
  comment = {related work; trust},
}


@InProceedings{Kerst_18,
  author    = {Kerstholt, Jos{\'e} and Barnhoorn, Jonathan and Hueting, Tom and Schuilenborg, Lotte},
  booktitle = {NATO-HAT Symposium on human autonomy teaming},
  title     = {Automation as an intelligent teammate: Social psychological implications},
  year      = {2018},
  comment   = {review},
}


@Article{Jiang_23,
  author    = {Jiang, Jinglu and Karran, Alexander J and Coursaris, Constantinos K and L{\'e}ger, Pierre-Majorique and Beringer, Joerg},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {A situation awareness perspective on human-AI interaction: Tensions and opportunities},
  year      = {2023},
  number    = {9},
  pages     = {1789--1806},
  volume    = {39},
  comment   = {related work},
  publisher = {Taylor \& Francis},
}


@Article{Israe_19,
  author     = {Israelsen, Brett W. and Ahmed, Nisar R.},
  journal    = {ACM Comput. Surv.},
  title      = {“Dave...I can assure you ...that it’s going to be all right ...” A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships},
  year       = {2019},
  issn       = {0360-0300},
  month      = {jan},
  number     = {6},
  volume     = {51},
  abstract   = {People who design, use, and are affected by autonomous artificially intelligent agents want to be able to trust such agents—that is, to know that these agents will perform correctly, to understand the reasoning behind their actions, and to know how to use them appropriately. Many techniques have been devised to assess and influence human trust in artificially intelligent agents. However, these approaches are typically ad hoc and have not been formally related to each other or to formal trust models. This article presents a survey of algorithmic assurances, i.e., programmed components of agent operation that are expressly designed to calibrate user trust in artificially intelligent agents. Algorithmic assurances are first formally defined and classified from the perspective of formally modeled human-artificially intelligent agent trust relationships. Building on these definitions, a synthesis of research across communities such as machine learning, human-computer interaction, robotics, e-commerce, and others reveals that assurance algorithms naturally fall along a spectrum in terms of their impact on an agent’s core functionality, with seven notable classes ranging from integral assurances (which impact an agent’s core functionality) to supplemental assurances (which have no direct effect on agent performance). Common approaches within each of these classes are identified and discussed; benefits and drawbacks of different approaches are also investigated.},
  address    = {New York, NY, USA},
  articleno  = {113},
  comment    = {review/related work},
  doi        = {10.1145/3267338},
  issue_date = {November 2019},
  keywords   = {transparency, interpretable machine learning, fairness, explainable artificial intelligence, algorithmic assurances, accountability, Human-computer trust},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3267338},
}



@Article{Humr_24,
  author  = {Humr, Scott A and Canan, Mustafa and Demir, Mustafa},
  journal = {arXiv preprint arXiv:2406.04956},
  title   = {Expansion of situations theory for exploring shared awareness in human-intelligent autonomous systems},
  year    = {2024},
  comment = {related work},
}

@Article{Hou_21,
  author    = {Hou, Ming and Ho, Geoffrey and Dunwoody, David},
  journal   = {Human-Intelligent Systems Integration},
  title     = {IMPACTS: A trust model for human-autonomy teaming},
  year      = {2021},
  number    = {2},
  pages     = {79--97},
  volume    = {3},
  comment   = {related work, trust},
  publisher = {Springer},
}

@InProceedings{Horst_23,
  author    = {Horstmann, Aike C and Strathmann, Clara and Lambrich, Lea and Kr{\"a}mer, Nicole C},
  booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
  title     = {Alexa, What's Inside of You: A Qualitative Study to Explore Users' Mental Models of Intelligent Voice Assistants},
  year      = {2023},
  pages     = {1--10},
  comment   = {related work},
}

@Misc{Gulat_23,
  author  = {Gulati, Aditya AND Lozano, Miguel Angel AND Lepri, Bruno AND Oliver, Nuria},
  title   = {BIASeD: Bringing Irrationality into Automated System Design},
  year    = {2023-01-26},
  comment = {related work; cognitive bias},
}

@Article{Gonza_23,
  author         = {González-Alday, Raquel and García-Cuesta, Esteban and Kulikowski, Casimir A. and Maojo, Victor},
  journal        = {Applied Sciences},
  title          = {A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine},
  year           = {2023},
  issn           = {2076-3417},
  number         = {19},
  volume         = {13},
  abstract       = {Due to the success of artificial intelligence (AI) applications in the medical field over the past decade, concerns about the explainability of these systems have increased. The reliability requirements of black-box algorithms for making decisions affecting patients pose a challenge even beyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integrating explainability into these systems. While most traditional AI methods and expert systems are inherently interpretable, the recent literature has focused primarily on explainability techniques for more complex models such as deep learning. This scoping review critically analyzes the existing literature regarding the explainability and interpretability of AI methods within the clinical domain. It offers a comprehensive overview of past and current research trends with the objective of identifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) in the field of medicine. Such constraints encompass the diverse requirements of key stakeholders, including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition, the absence of standardised evaluation criteria, the potential for mistaking explanations for causal relationships, and the apparent trade-off between model accuracy and interpretability. Furthermore, this review discusses possible research directions aimed at surmounting these challenges. These include alternative approaches to leveraging medical expertise to enhance interpretability within clinical settings, such as data fusion techniques and interdisciplinary assessments throughout the development process, emphasizing the relevance of taking into account the needs of final users to design trustable explainability methods.},
  article-number = {10778},
  comment        = {review (medical domain)},
  doi            = {10.3390/app131910778},
  url            = {https://www.mdpi.com/2076-3417/13/19/10778},
}


@Article{Fuchs_23,
  author    = {Fuchs, Andrew and Passarella, Andrea and Conti, Marco},
  journal   = {ACM Transactions on Autonomous and Adaptive Systems},
  title     = {Modeling, replicating, and predicting human behavior: a survey},
  year      = {2023},
  number    = {2},
  pages     = {1--47},
  volume    = {18},
  comment   = {related work},
  publisher = {ACM New York, NY, USA},
}

@Article{Endsl_15,
  author   = {Mica R. Endsley},
  journal  = {Journal of Cognitive Engineering and Decision Making},
  title    = {Situation Awareness Misconceptions and Misunderstandings},
  year     = {2015},
  number   = {1},
  pages    = {4-32},
  volume   = {9},
  abstract = {Situation awareness (SA) has become a widely used construct within the human factors community, the focus of considerable research over the past 25 years. This research has been used to drive the development of advanced information displays, the design of automated systems, information fusion algorithms, and new training approaches for improving SA in individuals and teams. In recent years, a number of papers criticized the Endsley model of SA on various grounds. I review those criticisms here and show them to be based on misunderstandings of the model. I also review several new models of SA, including situated SA, distributed SA, and sensemaking, in light of this discussion and show how they compare to existing models of SA in individuals and teams.},
  comment  = {related work},
  doi      = {10.1177/1555343415572631},
  eprint   = {https://doi.org/10.1177/1555343415572631},
  url      = {https://doi.org/10.1177/1555343415572631},
}



@Article{Dietz_22,
  author    = {Dietz, Emmanuelle and Kakas, Antonis and Michael, Loizos},
  journal   = {Frontiers in Artificial Intelligence},
  title     = {Argumentation: a calculus for human-centric AI},
  year      = {2022},
  pages     = {955579},
  volume    = {5},
  comment   = {related work; xai methods, form of explanations, more formal, refs with aduction paper},
  publisher = {Frontiers Media SA},
}

@InProceedings{Cotte_23,
  author       = {Cotter, Jenna E and Atchley, Andrew and Barr, Hannah M and Weger, Kristin and Mesmer, Bryan and Menon, Vineetha and Gholston, Sampson and Tenhundfeld, Nathan L},
  booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title        = {A New Framework for an Old Idea: Overhauling Reliability to Meet Current and Future Needs},
  year         = {2023},
  number       = {1},
  organization = {SAGE Publications Sage CA: Los Angeles, CA},
  pages        = {2062--2066},
  volume       = {67},
  comment      = {related work},
}

@Article{Corti_23,
  author   = {Cortiñas-Lorenzo, Karina and Lacey, Gerard},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Toward Explainable Affective Computing: A Review},
  year     = {2023},
  issn     = {2162-2388},
  pages    = {1-0},
  abstract = {Affective computing has an unprecedented potential to change the way humans interact with technology. While the last decades have witnessed vast progress in the field, multimodal affective computing systems are generally black box by design. As affective systems start to be deployed in real-world scenarios, such as education or healthcare, a shift of focus toward improved transparency and interpretability is needed. In this context, how do we explain the output of affective computing models? and how to do so without limiting predictive performance? In this article, we review affective computing work from an explainable AI (XAI) perspective, collecting and synthesizing relevant papers into three major XAI approaches: premodel (applied before training), in-model (applied during training), and postmodel (applied after training). We present and discuss the most fundamental challenges in the field, namely, how to relate explanations back to multimodal and time-dependent data, how to integrate context and inductive biases into explanations using mechanisms such as attention, generative modeling, or graph-based methods, and how to capture intramodal and cross-modal interactions in post hoc explanations. While explainable affective computing is still nascent, existing methods are promising, contributing not only toward improved transparency but, in many cases, surpassing state-of-the-art results. Based on these findings, we explore directions for future research and discuss the importance of data-driven XAI and explanation goals, and explainee needs definition, as well as causability or the extent to which a given method leads to human understanding.},
  comment  = {review},
  doi      = {10.1109/TNNLS.2023.3270027},
  keywords = {Affective computing;Training;Data models;Computational modeling;Task analysis;Terminology;Predictive models;Affective computing;explainable AI (XAI);multimodal machine learning;review},
}




@InProceedings{Corbe_23,
  author    = {Corbett, Eric and Denton, Emily},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  title     = {Interrogating the T in FAccT},
  year      = {2023},
  pages     = {1624--1634},
  comment   = {related work},
}


@InProceedings{Carli_22,
  author       = {Carli, Rachele and Najjar, Amro and Calvaresi, Davide},
  booktitle    = {International workshop on explainable, transparent autonomous agents and multi-agent systems},
  title        = {Risk and Exposure of XAI in Persuasion and Argumentation: The case of Manipulation},
  year         = {2022},
  organization = {Springer},
  pages        = {204--220},
  comment      = {related work; critical towards explanations only; aspect of manipulation},
}

@Article{Busch_23,
  author    = {Buschmeyer, Katharina and Hatfield, Sarah and Zenner, Julie},
  journal   = {Frontiers in Artificial Intelligence},
  title     = {Psychological assessment of AI-based decision support systems: tool development and expected benefits},
  year      = {2023},
  pages     = {1249322},
  volume    = {6},
  comment   = {related work; (study?)},
  publisher = {Frontiers Media SA},
}


@InProceedings{Brand_23,
  author       = {Brand, Lisa and Humm, Bernhard G and Krajewski, Andrea and Zender, Alexander},
  booktitle    = {International Conference on Engineering Applications of Neural Networks},
  title        = {Towards Improved User Experience for Artificial Intelligence Systems},
  year         = {2023},
  organization = {Springer},
  pages        = {33--44},
  comment      = {related work; UX design, user preference},
}


@Article{Bodri_23,
  author    = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
  journal   = {Data Mining and Knowledge Discovery},
  title     = {Benchmarking and survey of explanation methods for black box models},
  year      = {2023},
  number    = {5},
  pages     = {1719--1778},
  volume    = {37},
  comment   = {related work, xai methods},
  publisher = {Springer},
}


@InProceedings{Alix_21,
  author    = {Alix, Christophe and Lafond, Daniel and Mattioli, Juliette and Heer, Johan De and Chattington, Mark and Robic, Pierre-Olivier},
  booktitle = {2021 16th International Conference of System of Systems Engineering (SoSE)},
  title     = {Empowering Adaptive Human Autonomy Collaboration with Artificial Intelligence},
  year      = {2021},
  month     = {June},
  pages     = {126-131},
  abstract  = {We can now see examples emerge of intelligent distributed hybrid systems with autonomous functions pursuing a specific goal while adapting dynamically to changing environments. Such solutions are made possible by convergence of new technologies, but achieving comprehensive monitoring of the multiple interactions in their organization and functions along their life cycles, in missions and/or safety critical contexts, still challenges system (of systems) engineering practices. This paper considers the main gaps towards trusted systems of systems, including human-AI collaboration, human-machine teaming and solution effectiveness monitoring in a life cycle perspective. These gaps call for an inter-disciplinary sociotechnical approach in engineering towards Adjustable Human Autonomy Collaboration (DUAL), whose justification is outlined in this position paper.},
  comment   = {related work; trust, HAIT},
  doi       = {10.1109/SOSE52739.2021.9497497},
  keywords  = {Adaptive systems;Collaboration;Organizations;Safety;Artificial intelligence;Monitoring;Man-machine systems;Human machine teaming;autonomy;adaptive automation;trust;artificial intelligence},
}

@article{Page_21,
  title={The PRISMA 2020 statement: an updated guideline for reporting systematic reviews},
  author={Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and others},
  journal={bmj},
  volume={372},
  year={2021},
  publisher={British Medical Journal Publishing Group}
}

@article{Chou_22,
  title={Counterfactuals and causability in explainable artificial intelligence: Theory, algorithms, and applications},
  author={Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
  journal={Information Fusion},
  volume={81},
  pages={59--83},
  year={2022},
  publisher={Elsevier}
}

@article{Dazel_23,
  title={Explainable reinforcement learning for broad-xai: a conceptual framework and survey},
  author={Dazeley, Richard and Vamplew, Peter and Cruz, Francisco},
  journal={Neural Computing and Applications},
  volume={35},
  number={23},
  pages={16893--16916},
  year={2023},
  publisher={Springer}
}


@article{Gunni_19,
  title={XAI—Explainable artificial intelligence},
  author={Gunning, David and Stefik, Mark and Choi, Jaesik and Miller, Timothy and Stumpf, Simone and Yang, Guang-Zhong},
  journal={Science robotics},
  volume={4},
  number={37},
  pages={eaay7120},
  year={2019},
  publisher={American Association for the Advancement of Science}
}


@Article{Watso_88,
  author    = {Watson, David and Clark, Lee Anna and Tellegen, Auke},
  journal   = {Journal of personality and social psychology},
  title     = {Development and validation of brief measures of positive and negative affect: the PANAS scales.},
  year      = {1988},
  number    = {6},
  pages     = {1063},
  volume    = {54},
  comment   = {ref for PANAS scale},
  publisher = {American Psychological Association},
}

@Article{Sztaj_04,
  author  = {Sztajzel, Juan},
  journal = {Swiss medical weekly},
  title   = {Heart rate variability: a noninvasive electrocardiographic method to measure the autonomic nervous system},
  year    = {2004},
  number  = {3536},
  pages   = {514--522},
  volume  = {134},
  comment = {ref for HRV},
}

@Article{Paras_10,
  author    = {Parasuraman, Raja and Manzey, Dietrich H},
  journal   = {Human factors},
  title     = {Complacency and bias in human use of automation: An attentional integration},
  year      = {2010},
  number    = {3},
  pages     = {381--410},
  volume    = {52},
  comment   = {for ref in related section},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}


@Article{Taern_22a,
  author  = {T{\"a}rnholm, Therese and Liw{\aa}ng, Hans},
  journal = {Journal of Military Studies},
  title   = {Military organisations and emerging technologies--How do unmanned systems find a role in future navies?},
  year    = {2022},
  number  = {1},
  pages   = {37--48},
  volume  = {11},
}

@Article{Taern_22b,
  author  = {T{\"a}rnholm, Therese and Liw{\aa}ng, Hans},
  journal = {Journal of Maritime Research},
  title   = {Military autonomous underwater vehicles: An implementation perspective on legal and ethical aspects},
  year    = {2022},
  number  = {3},
  pages   = {39--46},
  volume  = {19},
}

@Article{Janai_20,
  author    = {Janai, Joel and G{\"u}ney, Fatma and Behl, Aseem and Geiger, Andreas and others},
  journal   = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  title     = {Computer vision for autonomous vehicles: Problems, datasets and state of the art},
  year      = {2020},
  number    = {1--3},
  pages     = {1--308},
  volume    = {12},
  publisher = {Now Publishers, Inc.},
}

@Article{Aradi_22,
  author    = {Aradi, Szil{\'a}rd},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  title     = {Survey of deep reinforcement learning for motion planning of autonomous vehicles},
  year      = {2020},
  number    = {2},
  pages     = {740--759},
  volume    = {23},
  publisher = {IEEE},
}

@Article{Kuutt_20,
  author    = {Kuutti, Sampo and Bowden, Richard and Jin, Yaochu and Barber, Phil and Fallah, Saber},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  title     = {A survey of deep learning applications to autonomous vehicle control},
  year      = {2020},
  number    = {2},
  pages     = {712--733},
  volume    = {22},
  publisher = {IEEE},
}

@Article{Morto_22,
  author    = {Morton, Jessica and Zheleva, Aleksandra and Van Acker, Bram B and Durnez, Wouter and Vanneste, Pieter and Larmuseau, Charlotte and De Bruyne, Jonas and Raes, Annelies and Cornillie, Frederik and Saldien, Jelle and others},
  journal   = {Applied Ergonomics},
  title     = {Danger, high voltage! Using EEG and EOG measurements for cognitive overload detection in a simulated industrial context},
  year      = {2022},
  pages     = {103763},
  volume    = {102},
  publisher = {Elsevier},
}

@Article{Omeiz_21,
  author    = {Omeiza, Daniel and Webb, Helena and Jirotka, Marina and Kunze, Lars},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  title     = {Explanations in autonomous driving: A survey},
  year      = {2021},
  number    = {8},
  pages     = {10142--10162},
  volume    = {23},
  publisher = {IEEE},
}

@InProceedings{Dorof_21,
  author       = {Doroftei, Daniela and De Vleeschauwer, Tom and Bue, Salvatore Lo and Dewyn, Micha{\"e}l and Vanderstraeten, Frik and De Cubber, Geert},
  booktitle    = {2021 30th IEEE International Conference on Robot \& Human Interactive Communication (RO-MAN)},
  title        = {Human-agent trust evaluation in a digital twin context},
  year         = {2021},
  organization = {IEEE},
  pages        = {203--207},
}

@Article{Chan_22,
  author    = {Chan, Jevon P and Norman, Rose and Pazouki, Kayvan and Golightly, David},
  journal   = {WMU Journal of Maritime Affairs},
  title     = {Autonomous maritime operations and the influence of situational awareness within maritime navigation},
  year      = {2022},
  number    = {2},
  pages     = {121--140},
  volume    = {21},
  publisher = {Springer},
}

@Article{Chan_23,
  author    = {Chan, Jevon Philip and Pazouki, Kayvan and Norman, Rosemary A},
  journal   = {Journal of Marine Engineering \& Technology},
  title     = {An experimental study into the fault recognition of onboard systems by navigational officers},
  year      = {2023},
  number    = {2},
  pages     = {101--110},
  volume    = {22},
  publisher = {Taylor \& Francis},
}

@Article{Pazou_18,
  author    = {Pazouki, Kayvan and Forbes, Neil and Norman, Rosemary A and Woodward, Michael D},
  journal   = {Ocean engineering},
  title     = {Investigation on the impact of human-automation interaction in maritime operations},
  year      = {2018},
  pages     = {297--304},
  volume    = {153},
  publisher = {Elsevier},
}

@InProceedings{Duym_22,
  author       = {Duym, Jens and Anwar, Ali and de Hoog, Jens and Mercelis, Siegfried and Hellinckx, Peter},
  booktitle    = {International Conference on P2P, Parallel, Grid, Cloud and Internet Computing},
  title        = {Improving Context-Aware Synthesis and Placement of Object Instances},
  year         = {2022},
  organization = {Springer},
  pages        = {288--296},
  comment      = {can we get paper from the authors?},
}

@InProceedings{Behza_23,
  author    = {Behzadi-Khormouji, Hamed and Oramas, Jos{\'e}},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  title     = {A protocol for evaluating model interpretation methods from visual explanations},
  year      = {2023},
  pages     = {1421--1429},
}

@InProceedings{Meyne_23,
  author       = {Meynen, Toon and Behzadi-Khormouji, Hamed and Oramas, Jos{\'e}},
  booktitle    = {2023 IEEE International Conference on Image Processing (ICIP)},
  title        = {Interpreting Convolutional Neural Networks by Explaining Their Predictions},
  year         = {2023},
  organization = {IEEE},
  pages        = {1685--1689},
  comment      = {check with IIT subscription},
}

@InProceedings{Tehra_16,
  author    = {Tehranchi, Farnaz and Ritter, Frank E},
  booktitle = {Proceedings of ICCM-2016-14th International Conference on Cognitive Modeling},
  title     = {Connecting Cognitive Models to Interact with Human-Computer Interfaces},
  year      = {2016},
}

@Article{Yuan_20,
  author    = {Yuan, Haiyue and Li, Shujun and Rusconi, Patrice and Yuan, Haiyue and Li, Shujun and Rusconi, Patrice},
  journal   = {Cognitive Modeling for Automated Human Performance Evaluation at Scale},
  title     = {Cognitive approaches to human computer interaction},
  year      = {2020},
  pages     = {5--15},
  comment   = {check paper with IIT subs},
  publisher = {Springer},
}

@Article{Monag_15,
  author    = {Monaghan, Conal and Bizumic, Boris and Reynolds, Katherine and Smithson, Michael and Johns-Boast, Lynette and Van Rooy, Dirk},
  journal   = {European Journal of Engineering Education},
  title     = {Performance of student software development teams: the influence of personality and identifying as team members},
  year      = {2015},
  number    = {1},
  pages     = {52--67},
  volume    = {40},
  publisher = {Taylor \& Francis},
}

@Article{Rooy_16,
  author    = {Van Rooy, Dirk and Wood, Ian and Tran, Eric},
  journal   = {Systems Research and Behavioral Science},
  title     = {Modelling the emergence of shared attitudes from group dynamics using an agent-based model of social comparison theory},
  year      = {2016},
  number    = {1},
  pages     = {188--204},
  volume    = {33},
  publisher = {Wiley Online Library},
}

@Article{Ritte_19,
  author    = {Ritter, Frank E and Tehranchi, Farnaz and Oury, Jacob D},
  journal   = {Wiley Interdisciplinary Reviews: Cognitive Science},
  title     = {ACT-R: A cognitive architecture for modeling cognition},
  year      = {2019},
  number    = {3},
  pages     = {e1488},
  volume    = {10},
  publisher = {Wiley Online Library},
}

@Article{Hanna_22,
  author    = {Hannaford, Elspeth and Maes, Pieter and Van Hassel, Edwin},
  journal   = {WMU Journal of Maritime Affairs},
  title     = {Autonomous ships and the collision avoidance regulations: a licensed deck officer survey},
  year      = {2022},
  number    = {2},
  pages     = {233--266},
  volume    = {21},
  publisher = {Springer},
}

@Article{Palla_24,
  author  = {Pallagani, Vishal and Roy, Kaushik and Muppasani, Bharath and Fabiano, Francesco and Loreggia, Andrea and Murugesan, Keerthiram and Srivastava, Biplav and Rossi, Francesca and Horesh, Lior and Sheth, Amit},
  journal = {arXiv preprint arXiv:2401.02500},
  title   = {On the prospects of incorporating large language models (llms) in automated planning and scheduling (aps)},
  year    = {2024},
}

@Article{Mir_22,
  author    = {Mir, Imran and Gul, Faiza and Mir, Suleman and Khan, Mansoor Ahmed and Saeed, Nasir and Abualigah, Laith and Abuhaija, Belal and Gandomi, Amir H},
  journal   = {Electronics},
  title     = {A survey of trajectory planning techniques for autonomous systems},
  year      = {2022},
  number    = {18},
  pages     = {2801},
  volume    = {11},
  publisher = {MDPI},
}

@Article{Dong_23,
  author    = {Dong, Lu and He, Zichen and Song, Chunwei and Sun, Changyin},
  journal   = {Journal of Systems Engineering and Electronics},
  title     = {A review of mobile robot motion planning methods: from classical motion planning workflows to reinforcement learning-based architectures},
  year      = {2023},
  number    = {2},
  pages     = {439--459},
  volume    = {34},
  publisher = {BIAI},
}

@Article{Moerl_23,
  author    = {Moerland, Thomas M and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M and others},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  title     = {Model-based reinforcement learning: A survey},
  year      = {2023},
  number    = {1},
  pages     = {1--118},
  volume    = {16},
  publisher = {Now Publishers, Inc.},
}

@Article{Zhang_21,
  author    = {Zhang, Lixian and Zhang, Ruixian and Wu, Tong and Weng, Rui and Han, Minghao and Zhao, Ye},
  journal   = {IEEE transactions on neural networks and learning systems},
  title     = {Safe reinforcement learning with stability guarantee for motion planning of autonomous vehicles},
  year      = {2021},
  number    = {12},
  pages     = {5435--5444},
  volume    = {32},
  comment   = {check paper with IIT sub},
  publisher = {IEEE},
}

@Article{Chen_21,
  author    = {Chen, Mo and Herbert, Sylvia L and Hu, Haimin and Pu, Ye and Fisac, Jaime Fernandez and Bansal, Somil and Han, SooJean and Tomlin, Claire J},
  journal   = {IEEE Transactions on Automatic Control},
  title     = {Fastrack: a modular framework for real-time motion planning and guaranteed safe tracking},
  year      = {2021},
  number    = {12},
  pages     = {5861--5876},
  volume    = {66},
  publisher = {IEEE},
}

@Article{Xue_23,
  author    = {Xue, Han and Ou, Yangjun},
  journal   = {Ocean Engineering},
  title     = {A novel asymmetric barrier Lyapunov function-based fixed-time ship berthing control under multiple state constraints},
  year      = {2023},
  pages     = {114756},
  volume    = {281},
  comment   = {check paper with IIT subs},
  publisher = {Elsevier},
}

@Article{Zhang_19,
  author  = {Zhang, Yujia and Song, Kuangyan and Sun, Yiming and Tan, Sarah and Udell, Madeleine},
  journal = {arXiv preprint arXiv:1904.12991},
  title   = {" Why should you trust my explanation?" understanding uncertainty in LIME explanations},
  year    = {2019},
}

@Article{Riber_16a,
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1606.05386},
  title   = {Model-agnostic interpretability of machine learning},
  year    = {2016},
}

@Article{Rahna_19,
  author  = {Rahnama, Amir Hossein Akhavan and Bostr{\"o}m, Henrik},
  journal = {arXiv preprint arXiv:1910.14421},
  title   = {A study of data and label shift in the LIME framework},
  year    = {2019},
}

@Article{Ribei_16b,
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1611.05817},
  title   = {Nothing else matters: Model-agnostic explanations by identifying prediction invariance},
  year    = {2016},
}

@Article{Lundb_17,
  author  = {Lundberg, Scott M and Lee, Su-In},
  journal = {Advances in neural information processing systems},
  title   = {A unified approach to interpreting model predictions},
  year    = {2017},
  volume  = {30},
}

@Article{Adeba_18a,
  author  = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  journal = {Advances in neural information processing systems},
  title   = {Sanity checks for saliency maps},
  year    = {2018},
  volume  = {31},
}

@Article{Adeba_18b,
  author  = {Adebayo, Julius and Gilmer, Justin and Goodfellow, Ian and Kim, Been},
  journal = {arXiv preprint arXiv:1810.03307},
  title   = {Local explanation methods for deep neural networks lack sensitivity to parameter values},
  year    = {2018},
}

@Article{Felzm_20,
  author    = {Felzmann, Heike and Fosch-Villaronga, Eduard and Lutz, Christoph and Tam{\`o}-Larrieux, Aurelia},
  journal   = {Science and engineering ethics},
  title     = {Towards transparency by design for artificial intelligence},
  year      = {2020},
  number    = {6},
  pages     = {3333--3361},
  volume    = {26},
  publisher = {Springer},
}

@Article{Schoo_21,
  author    = {Schoonderwoerd, Tjeerd AJ and Jorritsma, Wiard and Neerincx, Mark A and Van Den Bosch, Karel},
  journal   = {International Journal of Human-Computer Studies},
  title     = {Human-centered XAI: Developing design patterns for explanations of clinical decision support systems},
  year      = {2021},
  pages     = {102684},
  volume    = {154},
  publisher = {Elsevier},
}

@Article{Cronh_22,
  author = {Cronholm, Stefan and G{\"o}bel, Hannes},
  title  = {Design principles for human-centred AI},
  year   = {2022},
}

@Article{Shnei_20a,
  author  = {Shneiderman, Ben},
  journal = {AIS Transactions on Human-Computer Interaction},
  title   = {Human-centered artificial intelligence: Three fresh ideas},
  year    = {2020},
  number  = {3},
  pages   = {109--124},
  volume  = {12},
}

@Article{Shnei_20b,
  author    = {Shneiderman, Ben},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Human-centered artificial intelligence: Reliable, safe \& trustworthy},
  year      = {2020},
  number    = {6},
  pages     = {495--504},
  volume    = {36},
  publisher = {Taylor \& Francis},
}

@Article{Shnei_20c,
  author    = {Shneiderman, Ben},
  journal   = {IEEE Transactions on Technology and Society},
  title     = {Design lessons from AI’s two grand goals: human emulation and useful applications},
  year      = {2020},
  number    = {2},
  pages     = {73--82},
  volume    = {1},
  publisher = {IEEE},
}

@InProceedings{Vanne_22,
  author       = {Vanneste, Astrid and Vanneste, Simon and Vasseur, Olivier and Janssens, Robin and Billast, Mattias and Anwar, Ali and Mets, Kevin and De Schepper, Tom and Mercelis, Siegfried and Hellinckx, Peter},
  booktitle    = {IECON 2022--48th Annual Conference of the IEEE Industrial Electronics Society},
  title        = {Safety aware autonomous path planning using model predictive reinforcement learning for inland waterways},
  year         = {2022},
  organization = {IEEE},
  pages        = {1--6},
  comment      = {check paper with IIT subs},
}

@Article{Guo_20,
  author    = {Guo, Siyu and Zhang, Xiuguo and Zheng, Yisong and Du, Yiquan},
  journal   = {Sensors},
  title     = {An autonomous path planning model for unmanned ships based on deep reinforcement learning},
  year      = {2020},
  number    = {2},
  pages     = {426},
  volume    = {20},
  publisher = {MDPI},
}

@Article{Vagal_21,
  author    = {Vagale, Anete and Bye, Robin T and Oucheikh, Rachid and Osen, Ottar L and Fossen, Thor I},
  journal   = {Journal of Marine Science and Technology},
  title     = {Path planning and collision avoidance for autonomous surface vehicles II: a comparative study of algorithms},
  year      = {2021},
  number    = {4},
  pages     = {1307--1323},
  volume    = {26},
  publisher = {Springer},
}

@InProceedings{Lu_24,
  author       = {Lu, Wenhao and Zhao, Xufeng and Fryen, Thilo and Lee, Jae Hee and Li, Mengdi and Magg, Sven and Wermter, Stefan},
  booktitle    = {Causal Learning and Reasoning},
  title        = {Causal State Distillation for Explainable Reinforcement Learning},
  year         = {2024},
  organization = {PMLR},
  pages        = {106--142},
}

@InProceedings{Amita_24,
  author    = {Amitai, Yotam and Septon, Yael and Amir, Ofra},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Explaining Reinforcement Learning Agents through Counterfactual Action Outcomes},
  year      = {2024},
  number    = {9},
  pages     = {10003--10011},
  volume    = {38},
}

@Article{Madum_20a,
  author  = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  journal = {arXiv preprint arXiv:2001.10284},
  title   = {Distal explanations for model-free explainable reinforcement learning},
  year    = {2020},
}

@Article{Milan_23,
  author    = {Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal   = {ACM Computing Surveys},
  title     = {Explainable reinforcement learning: A survey and comparative review},
  year      = {2023},
  publisher = {ACM New York, NY},
}

@Article{Xiong_24,
  author  = {Xiong, Yu and Hu, Zhipeng and Huang, Ye and Wu, Runze and Guan, Kai and Fang, Xingchen and Jiang, Ji and Zhou, Tianze and Hu, Yujing and Liu, Haoyu and others},
  journal = {arXiv preprint arXiv:2402.12685},
  title   = {XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques},
  year    = {2024},
}

@Article{Madum_20b,
  author  = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  journal = {arXiv preprint arXiv:2001.10284},
  title   = {Distal explanations for explainable reinforcement learning agents},
  year    = {2020},
  volume  = {2},
}

@Article{Gajci_24,
  author  = {Gajcin, Jasmina and Dusparic, Ivana},
  journal = {arXiv preprint arXiv:2402.06503},
  title   = {ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies},
  year    = {2024},
}

@Article{Bekke_24,
  author    = {Bekkemoen, Yanzhe},
  journal   = {Machine Learning},
  title     = {Explainable reinforcement learning (XRL): a systematic literature review and taxonomy},
  year      = {2024},
  number    = {1},
  pages     = {355--441},
  volume    = {113},
  publisher = {Springer},
}

@Article{Mishr_21,
  author  = {Aditi Mishra and Utkarsh Soni and Jinbin Huang and Chris Bryan},
  journal = {2022 IEEE 15th Pacific Visualization Symposium (PacificVis)},
  title   = {Why? Why not? When? Visual Explanations of Agent Behaviour in Reinforcement Learning},
  year    = {2021},
  pages   = {111-120},
  url     = {https://api.semanticscholar.org/CorpusID:233168588},
}

@Article{Krajn_22,
  author  = {Krajna, Agneza and Brcic, Mario and Lipic, Tomislav and Doncevic, Juraj},
  journal = {arXiv preprint arXiv:2203.11547},
  title   = {Explainability in reinforcement learning: perspective and position},
  year    = {2022},
}

@Article{Wang_23,
  author    = {Wang, Xiaoxiao and Meng, Fanyu and Liu, Xin and Kong, Zhaodan and Chen, Xin},
  journal   = {Applied Intelligence},
  title     = {Causal explanation for reinforcement learning: Quantifying state and temporal importance},
  year      = {2023},
  number    = {19},
  pages     = {22546--22564},
  volume    = {53},
  publisher = {Springer},
}

@Article{Legoo_23,
  author    = {Legood, Alison and van der Werff, Lisa and Lee, Allan and den Hartog, Deanne and van Knippenberg, Daan},
  journal   = {Journal of Management Studies},
  title     = {A critical review of the conceptualization, operationalization, and empirical literature on cognition-based and affect-based trust},
  year      = {2023},
  number    = {2},
  pages     = {495--537},
  volume    = {60},
  publisher = {Wiley Online Library},
}

@Article{Alexa_18,
  author    = {Alexander, Veronika and Blinder, Collin and Zak, Paul J},
  journal   = {Computers in Human Behavior},
  title     = {Why trust an algorithm? Performance, cognition, and neurophysiology},
  year      = {2018},
  pages     = {279--288},
  volume    = {89},
  publisher = {Elsevier},
}

@Article{Webbe_08,
  author    = {Webber, Sheila Simsarian},
  journal   = {Small group research},
  title     = {Development of cognitive and affective trust in teams: A longitudinal study},
  year      = {2008},
  number    = {6},
  pages     = {746--769},
  volume    = {39},
  comment   = {check paper with IIT subs},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{Spect_04,
  author    = {Spector, Michele D and Jones, Gwen E},
  journal   = {The Journal of social psychology},
  title     = {Trust in the workplace: Factors affecting trust formation between team members},
  year      = {2004},
  number    = {3},
  pages     = {311--321},
  volume    = {144},
  publisher = {Taylor \& Francis},
}

@Article{Frazi_13,
  author    = {Frazier, M Lance and Johnson, Paul D and Fainshmidt, Stav},
  journal   = {Journal of Trust Research},
  title     = {Development and validation of a propensity to trust scale},
  year      = {2013},
  number    = {2},
  pages     = {76--97},
  volume    = {3},
  publisher = {Taylor \& Francis},
}

@InProceedings{Adams_20,
  author    = {Adams, Barbara D and Webb, Robert},
  booktitle = {7th international command and control technology symposium},
  title     = {Trust in small military teams},
  year      = {2002},
  pages     = {1--20},
}

@Article{Kiffi_04,
  author    = {Kiffin-Petersen, Sandra},
  journal   = {Journal of Management \& Organization},
  title     = {Trust: A neglected variable in team effectiveness research},
  year      = {2004},
  number    = {1},
  pages     = {38--53},
  volume    = {10},
  publisher = {Cambridge University Press},
}

@Article{Bucin_21,
  author    = {Bu{\c{c}}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z},
  journal   = {Proceedings of the ACM on Human-Computer Interaction},
  title     = {To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making},
  year      = {2021},
  number    = {CSCW1},
  pages     = {1--21},
  volume    = {5},
  publisher = {ACM New York, NY, USA},
}

@InProceedings{Wang_19,
  author    = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y},
  booktitle = {Proceedings of the 2019 CHI conference on human factors in computing systems},
  title     = {Designing theory-driven user-centric explainable AI},
  year      = {2019},
  pages     = {1--15},
}

@Article{Liao_22,
  author  = {Liao, Q Vera and Varshney, Kush R},
  journal = {arXiv preprint arXiv:2110.10790},
  title   = {Human-centered explainable ai (xai): From algorithms to user experiences},
  year    = {2022},
}

@Article{Rawal_21,
  author    = {Rawal, Atul and McCoy, James and Rawat, Danda B and Sadler, Brian M and Amant, Robert St},
  journal   = {IEEE Transactions on Artificial Intelligence},
  title     = {Recent advances in trustworthy explainable artificial intelligence: Status, challenges, and perspectives},
  year      = {2021},
  number    = {6},
  pages     = {852--866},
  volume    = {3},
  publisher = {IEEE},
}

@Article{Mohse_21,
  author    = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D},
  journal   = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
  title     = {A multidisciplinary survey and framework for design and evaluation of explainable AI systems},
  year      = {2021},
  number    = {3-4},
  pages     = {1--45},
  volume    = {11},
  publisher = {ACM New York, NY},
}

@Article{Ehsan_24,
  author  = {Ehsan, Upol and Passi, Samir and Liao, Q Vera and Chan, Larry and Lee, I and Muller, Michael and Riedl, Mark O and others},
  journal = {arXiv preprint arXiv:2107.13509},
  title   = {The who in explainable ai: How ai background shapes perceptions of ai explanations},
  year    = {2021},
}

@Article{Taylo_21,
  author    = {Taylor, J Eric T and Taylor, Graham W},
  journal   = {Psychonomic Bulletin \& Review},
  title     = {Artificial cognition: How experimental psychology can help generate explainable artificial intelligence},
  year      = {2021},
  number    = {2},
  pages     = {454--475},
  volume    = {28},
  publisher = {Springer},
}

@InProceedings{Torei_20,
  author    = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and Van Moorsel, Aad},
  booktitle = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
  title     = {The relationship between trust in AI and trustworthy machine learning technologies},
  year      = {2020},
  pages     = {272--283},
}

@Article{Schoe_23,
  author    = {Schoenherr, Jordan Richard and Abbas, Roba and Michael, Katina and Rivas, Pablo and Anderson, Theresa Dirndorfer},
  journal   = {IEEE Transactions on Technology and Society},
  title     = {Designing AI using a human-centered approach: Explainability and accuracy toward trustworthiness},
  year      = {2023},
  number    = {1},
  pages     = {9--23},
  volume    = {4},
  publisher = {IEEE},
}

@Article{Schel_22a,
  author    = {Schelble, Beau G and Flathmann, Christopher and McNeese, Nathan J and Freeman, Guo and Mallick, Rohit},
  journal   = {Proceedings of the ACM on Human-Computer Interaction},
  title     = {Let's think together! Assessing shared mental models, performance, and trust in human-agent teams},
  year      = {2022},
  number    = {GROUP},
  pages     = {1--29},
  volume    = {6},
  publisher = {ACM New York, NY, USA},
}

@InProceedings{Neeri_18,
  author       = {Neerincx, Mark A and van der Waa, Jasper and Kaptein, Frank and van Diggelen, Jurriaan},
  booktitle    = {Engineering Psychology and Cognitive Ergonomics: 15th International Conference, EPCE 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings 15},
  title        = {Using perceptual and cognitive explanations for enhanced human-agent team performance},
  year         = {2018},
  organization = {Springer},
  pages        = {204--214},
}

@Article{Descl_22,
  author       = {Desclaux, Gilles},
  journal      = {Bernard Claverie; Baptiste Prébot; Norbou Buchler; François du Cluzel. Cognitive Warfare: TheFuture of Cognitive Dominance,},
  title        = {Trust Between Humans and Intelligent Machines and Induced Cognitive Biases},
  year         = {2022},
  pages        = {1-5},
  howpublished = {HAL Science},
}

@Article{Sanne_22,
  author    = {Sanneman, Lindsay and Shah, Julie A},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {The situation awareness framework for explainable AI (SAFE-AI) and human factors considerations for XAI systems},
  year      = {2022},
  number    = {18-20},
  pages     = {1772--1788},
  volume    = {38},
  publisher = {Taylor \& Francis},
}

@Article{Andre_22,
  author    = {Andrews, Robert W and Lilly, J Mason and Srivastava, Divya and Feigh, Karen M},
  journal   = {Theoretical Issues in Ergonomics Science},
  title     = {The role of shared mental models in human-AI teams: a theoretical review},
  year      = {2023},
  number    = {2},
  pages     = {129--175},
  volume    = {24},
  publisher = {Taylor \& Francis},
}

@InProceedings{Branl_20,
  author       = {Branley-Bell, Dawn and Whitworth, Rebecca and Coventry, Lynne},
  booktitle    = {International Conference on Human-Computer Interaction},
  title        = {User trust and understanding of explainable AI: exploring algorithm visualisations and user biases},
  year         = {2020},
  organization = {Springer},
  pages        = {382--399},
  comment      = {check paper with IIT subs},
}

@Article{Kohn_21,
  author    = {Kohn, Spencer C and De Visser, Ewart J and Wiese, Eva and Lee, Yi-Ching and Shaw, Tyler H},
  journal   = {Frontiers in psychology},
  title     = {Measurement of trust in automation: A narrative review and reference guide},
  year      = {2021},
  pages     = {604977},
  volume    = {12},
  publisher = {Frontiers},
}

@Article{Jian_00,
  author    = {Jian, Jiun-Yin and Bisantz, Ann M and Drury, Colin G},
  journal   = {International journal of cognitive ergonomics},
  title     = {Foundations for an empirically determined scale of trust in automated systems},
  year      = {2000},
  number    = {1},
  pages     = {53--71},
  volume    = {4},
  publisher = {Taylor \& Francis},
}

@Article{Mayer_95,
  author    = {Mayer, Roger C and Davis, James H and Schoorman, F David},
  journal   = {Academy of management review},
  title     = {An integrative model of organizational trust},
  year      = {1995},
  number    = {3},
  pages     = {709--734},
  volume    = {20},
  publisher = {Academy of Management Briarcliff Manor, NY 10510},
}

@Article{Merri_19,
  author    = {Merritt, Stephanie M and Ako-Brew, Alicia and Bryant, William J and Staley, Amy and McKenna, Michael and Leone, Austin and Shirase, Lei},
  journal   = {Frontiers in psychology},
  title     = {Automation-induced complacency potential: Development and validation of a new scale},
  year      = {2019},
  pages     = {225},
  volume    = {10},
  publisher = {Frontiers Media SA},
}

@Article{Schae_16a,
  author    = {Schaefer, Kristin E and Chen, Jessie YC and Szalma, James L and Hancock, Peter A},
  journal   = {Human factors},
  title     = {A meta-analysis of factors influencing the development of trust in automation: Implications for understanding autonomy in future systems},
  year      = {2016},
  number    = {3},
  pages     = {377--400},
  volume    = {58},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{Riege_23,
  author    = {Rieger, Tobias and Kugler, Luisa and Manzey, Dietrich and Roesler, Eileen},
  journal   = {Human Factors},
  title     = {The (Im) perfect Automation Schema: Who Is Trusted More, Automated or Human Decision Support?},
  year      = {2023},
  pages     = {00187208231197347},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Wiczo_19,
  author    = {Wiczorek, Rebecca and Meyer, Joachim},
  journal   = {Frontiers in psychology},
  title     = {Effects of trust, self-confidence, and feedback on the use of decision automation},
  year      = {2019},
  pages     = {418075},
  volume    = {10},
  publisher = {Frontiers},
}

@Article{Merri_08,
  author    = {Merritt, Stephanie M and Ilgen, Daniel R},
  journal   = {Human factors},
  title     = {Not all trust is created equal: Dispositional and history-based trust in human-automation interactions},
  year      = {2008},
  number    = {2},
  pages     = {194--210},
  volume    = {50},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Frank_19,
  author    = {Franke, Thomas and Attig, Christiane and Wessel, Daniel},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {A personal resource for technology interaction: development and validation of the affinity for technology interaction (ATI) scale},
  year      = {2019},
  number    = {6},
  pages     = {456--467},
  volume    = {35},
  publisher = {Taylor \& Francis},
}

@Article{Rasto_22,
  author    = {Rastogi, Charvi and Zhang, Yunfeng and Wei, Dennis and Varshney, Kush R and Dhurandhar, Amit and Tomsett, Richard},
  journal   = {Proceedings of the ACM on Human-computer Interaction},
  title     = {Deciding fast and slow: The role of cognitive biases in AI-assisted decision-making},
  year      = {2022},
  number    = {CSCW1},
  pages     = {1--22},
  volume    = {6},
  publisher = {ACM New York, NY, USA},
}

@InProceedings{Berna_23,
  author       = {Bernault, Chlo{\'e} and Juan, Sara and Delmas, Alexandra and Andre, Jean-Marc and Rodier, Marc and Chraibi Kaadoud, Ikram},
  booktitle    = {International Conference on Human-Computer Interaction},
  title        = {Assessing the impact of cognitive biases in AI project development},
  year         = {2023},
  organization = {Springer},
  pages        = {401--420},
}

@InProceedings{Bertr_22,
  author    = {Bertrand, Astrid and Belloum, Rafik and Eagan, James R and Maxwell, Winston},
  booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  title     = {How cognitive biases affect XAI-assisted decision-making: A systematic review},
  year      = {2022},
  pages     = {78--91},
}

@InProceedings{Santh_20,
  author    = {Santhanam, Sashank and Karduni, Alireza and Shaikh, Samira},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  title     = {Studying the effects of cognitive biases in evaluation of conversational agents},
  year      = {2020},
  pages     = {1--13},
}

@Article{Hoffm_23,
  author    = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
  journal   = {Frontiers in Computer Science},
  title     = {Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance},
  year      = {2023},
  pages     = {1096257},
  volume    = {5},
  comment   = {Both trust and XAI},
  publisher = {Frontiers Media SA},
}

@Article{Zeril_22,
  author    = {Zerilli, John and Bhatt, Umang and Weller, Adrian},
  journal   = {Patterns},
  title     = {How transparency modulates trust in artificial intelligence},
  year      = {2022},
  number    = {4},
  volume    = {3},
  publisher = {Elsevier},
}

@Article{Texto_22,
  author    = {Textor, Claire and Zhang, Rui and Lopez, Jeremy and Schelble, Beau G and McNeese, Nathan J and Freeman, Guo and Pak, Richard and Tossell, Chad and de Visser, Ewart J},
  journal   = {Journal of cognitive engineering and decision making},
  title     = {Exploring the relationship between ethics and trust in human--artificial intelligence teaming: A mixed methods approach},
  year      = {2022},
  number    = {4},
  pages     = {252--281},
  volume    = {16},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Papag_22,
  author    = {Papagni, Guglielmo and de Pagter, Jesse and Zafari, Setareh and Filzmoser, Michael and Koeszegi, Sabine T},
  journal   = {Ai \& Society},
  title     = {Artificial agents’ explainability to support trust: considerations on timing and context},
  year      = {2023},
  number    = {2},
  pages     = {947--960},
  volume    = {38},
  publisher = {Springer},
}

@Article{Ha_24,
  author    = {Ha, Taehyun and Kim, Sangyeon},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Improving Trust in AI with Mitigating Confirmation Bias: Effects of Explanation Type and Debiasing Strategy for Decision-Making with Explainable AI},
  year      = {2024},
  pages     = {1--12},
  publisher = {Taylor \& Francis},
}

@article{Hoff_15,
  title={Trust in automation: Integrating empirical evidence on factors that influence trust},
  author={Hoff, Kevin Anthony and Bashir, Masooda},
  journal={Human factors},
  volume={57},
  number={3},
  pages={407--434},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@Article{Dorto_22,
  author    = {Dorton, Stephen L and Harper, Samantha B},
  journal   = {Journal of Cognitive Engineering and Decision Making},
  title     = {A naturalistic investigation of trust, AI, and intelligence work},
  year      = {2022},
  number    = {4},
  pages     = {222--236},
  volume    = {16},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Locke_21,
  author = {Lockey, S., Gillespie, N., Holm, D. and Someh, I.A},
  title  = {A review of trust in artificial intelligence: Challenges, vulnerabilities and future directions},
  year   = {2021},
}


@Article{Weber_23,
  author    = {Weber, Leander and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech},
  journal   = {Information Fusion},
  title     = {Beyond explaining: Opportunities and challenges of XAI-based model improvement},
  year      = {2023},
  pages     = {154--176},
  volume    = {92},
  publisher = {Elsevier},
}


@Article{Arrie_20,
  author    = {Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal   = {Information fusion},
  title     = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  year      = {2020},
  pages     = {82--115},
  volume    = {58},
  publisher = {Elsevier},
}

@Article{Ali_23,
  author    = {Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and D{\'\i}az-Rodr{\'\i}guez, Natalia and Herrera, Francisco},
  journal   = {Information fusion},
  title     = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
  year      = {2023},
  pages     = {101805},
  volume    = {99},
  publisher = {Elsevier},
}

@Article{Heuil_20,
  author    = {Heuillet, Alexandre and Couthouis, Fabien and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal   = {Knowledge-Based Systems},
  title     = {Explainability in deep reinforcement learning},
  year      = {2021},
  pages     = {106685},
  volume    = {214},
  publisher = {Elsevier},
}

@Article{Pias_24,
  author  = {Pias, Sabid Bin Habib and Freel, Alicia and Trammel, Timothy and Akter, Taslima and Williamson, Donald and Kapadia, Apu},
  journal = {arXiv preprint arXiv:2404.19629, extended abstract SIGCHI},
  title   = {The Drawback of Insight: Detailed Explanations Can Reduce Agreement with XAI},
  year    = {2024},
  comment = {(extended abstract to SIGCHI 24): shows correlation between personality traits and explanations},
}

@Article{Adadi_18,
  author    = {Adadi, Amina and Berrada, Mohammed},
  journal   = {IEEE access},
  title     = {Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)},
  year      = {2018},
  pages     = {52138--52160},
  volume    = {6},
  publisher = {IEEE},
}

@Article{Das_20,
  author  = {Das, Arun and Rad, Paul},
  journal = {arXiv preprint arXiv:2006.11371},
  title   = {Opportunities and challenges in explainable artificial intelligence (xai): A survey},
  year    = {2020},
}

@Article{Lopes_22,
  author    = {Lopes, Pedro and Silva, Eduardo and Braga, Cristiana and Oliveira, Tiago and Rosado, Lu{\'\i}s},
  journal   = {Applied Sciences},
  title     = {XAI systems evaluation: A review of human and computer-centred methods},
  year      = {2022},
  number    = {19},
  pages     = {9423},
  volume    = {12},
  comment   = {quite exhaustive already from the human side, check for gaps},
  publisher = {MDPI},
}

@Article{Chamo_23,
  author    = {Chamola, Vinay and Hassija, Vikas and Sulthana, A Razia and Ghosh, Debshishu and Dhingra, Divyansh and Sikdar, Biplab},
  journal   = {IEEE Access},
  title     = {A review of trustworthy and explainable artificial intelligence (xai)},
  year      = {2023},
  comment   = {not sure about novelty},
  publisher = {IEEE},
}

@Article{Kelly_23,
  author    = {Kelly, Sage and Kaye, Sherrie-Anne and Oviedo-Trespalacios, Oscar},
  journal   = {Telematics and Informatics},
  title     = {What factors contribute to the acceptance of artificial intelligence? A systematic review},
  year      = {2023},
  pages     = {101925},
  volume    = {77},
  publisher = {Elsevier},
}

@Article{Kahko_20,
  author    = {K{\"a}hk{\"o}nen, Tiina and Vanhala, Mika and Blomqvist, Kirsimarja},
  journal   = {Journal of Advances in Management Research},
  title     = {Employee trust-repair practices: scale development and validation},
  year      = {2023},
  number    = {3},
  pages     = {539--563},
  volume    = {20},
  comment   = {more related to employee but applied employee trust-repair scale (ETRS) to members of Finnish army. Provides theoretical section to trust repair.},
  publisher = {Emerald Publishing Limited},
}

@Article{Lopez_23,
  author    = {Lopez, Jeremy and Textor, Claire and Lancaster, Caitlin and Schelble, Beau and Freeman, Guo and Zhang, Rui and McNeese, Nathan and Pak, Richard},
  journal   = {AI and Ethics},
  title     = {The complex relationship of AI ethics and trust in human--AI teaming: insights from advanced real-world subject matter experts},
  year      = {2023},
  pages     = {1--21},
  publisher = {Springer},
}

@Article{Kong_24,
  author  = {Kong, Xiangqi and Xing, Yang and Tsourdos, Antonios and Wang, Ziyue and Guo, Weisi and Perrusquia, Adolfo and Wikander, Andreas},
  journal = {arXiv preprint arXiv:2405.02583},
  title   = {Explainable Interface for Human-Autonomy Teaming: A Survey},
  year    = {2024},
  comment = {not cited yet, check for publication (DOI)},
}


@Article{Kox_22,
  author    = {Kox, Esther Siegling and Siegling, LB and Kerstholt, Jose H},
  journal   = {International journal of social robotics},
  title     = {Trust development in military and civilian human--agent teams: the effect of social-cognitive recovery strategies},
  year      = {2022},
  number    = {5},
  pages     = {1323--1338},
  volume    = {14},
  publisher = {Springer},
}

@Article{Demir_21,
  author    = {Demir, Mustafa and McNeese, Nathan J and Gorman, Jaime C and Cooke, Nancy J and Myers, Christopher W and Grimm, David A},
  journal   = {IEEE transactions on human-machine systems},
  title     = {Exploration of teammate trust and interaction dynamics in human-autonomy teaming},
  year      = {2021},
  number    = {6},
  pages     = {696--705},
  volume    = {51},
  publisher = {IEEE},
}

@Article{Chen_18,
  author    = {Chen, Jessie YC and Lakhmani, Shan G and Stowers, Kimberly and Selkowitz, Anthony R and Wright, Julia L and Barnes, Michael},
  journal   = {Theoretical issues in ergonomics science},
  title     = {Situation awareness-based agent transparency and human-autonomy teaming effectiveness},
  year      = {2018},
  number    = {3},
  pages     = {259--282},
  volume    = {19},
  publisher = {Taylor \& Francis},
}

@Article{Endsl_23,
  author    = {Endsley, Mica R},
  journal   = {Computers in Human Behavior},
  title     = {Supporting Human-AI Teams: Transparency, explainability, and situation awareness},
  year      = {2023},
  pages     = {107574},
  volume    = {140},
  publisher = {Elsevier},
}


@article{barto2021reinforcement,
  title={Reinforcement learning: An introduction by Richards’ Sutton},
  author={Barto, Andrew G},
  journal={SIAM Rev},
  volume={6},
  number={2},
  pages={423},
  year={2021},
  publisher={SIAM}
}
@book{Sutto_18,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@book{Barto_89,
  title={Learning and sequential decision making},
  author={Barto, Andrew Gehret and Sutton, Richard S and Watkins, CJCH},
  volume={89},
  year={1989},
  publisher={University of Massachusetts Amherst, MA}
}


@article{Schil_21,
  title={Trust in social relations},
  author={Schilke, Oliver and Reimann, Martin and Cook, Karen S},
  journal={Annual Review of Sociology},
  volume={47},
  pages={239--259},
  year={2021},
  publisher={Annual Reviews}
}

@article{Law_21,
  title={Trust: Recent concepts and evaluations in human-robot interaction},
  author={Law, Theresa and Scheutz, Matthias},
  journal={Trust in human-robot interaction},
  pages={27--57},
  year={2021},
  publisher={Elsevier}
}

@article{Comin_21,
  title={Promises and trust in human--robot interaction},
  author={Cominelli, Lorenzo and Feri, Francesco and Garofalo, Roberto and Giannetti, Caterina and Mel{\'e}ndez-Jim{\'e}nez, Miguel A and Greco, Alberto and Nardelli, Mimma and Scilingo, Enzo Pasquale and Kirchkamp, Oliver},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={9687},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{Hanco_11,
  title={A meta-analysis of factors affecting trust in human-robot interaction},
  author={Hancock, Peter A and Billings, Deborah R and Schaefer, Kristin E and Chen, Jessie YC and De Visser, Ewart J and Parasuraman, Raja},
  journal={Human factors},
  volume={53},
  number={5},
  pages={517--527},
  year={2011},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{Berre_23,
  title={Defining human-AI teaming the human-centered way: a scoping review and network analysis},
  author={Berretta, Sophie and Tausch, Alina and Ontrup, Greta and Gilles, Bj{\"o}rn and Peifer, Corinna and Kluge, Annette},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  year={2023},
  publisher={Frontiers Media SA}
}

@article{Hemme_21,
  title={Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.},
  author={Hemmer, Patrick and Schemmer, Max and V{\"o}ssing, Michael and K{\"u}hl, Niklas},
  journal={PACIS},
  pages={78},
  year={2021}
}

@article{Mehro_23,
  title={A Systematic Review on Fostering Appropriate Trust in Human-AI Interaction},
  author={Mehrotra, Siddharth and Degachi, Chadha and Vereschak, Oleksandra and Jonker, Catholijn M and Tielman, Myrthe L},
  journal={arXiv preprint arXiv:2311.06305},
  year={2023}
}


@article{Zerch_23,
  title={When AI joins the team: A literature review on intragroup processes and their effect on team performance in team-AI collaboration},
  author={Zercher, D{\'e}sir{\'e}e and Jussupow, Ekaterina and Heinzl, Armin},
  year={2023}
}


@inproceedings{Ueno_22,
  title={Trust in human-ai interaction: Scoping out models, measures, and methods},
  author={Ueno, Takane and Sawa, Yuto and Kim, Yeongdae and Urakami, Jacqueline and Oura, Hiroki and Seaborn, Katie},
  booktitle={CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages={1--7},
  year={2022}
}

@article{Bao_23,
  title={A Literature Review of Human--AI Synergy in Decision Making: From the Perspective of Affordance Actualization Theory},
  author={Bao, Ying and Gong, Wankun and Yang, Kaiwen},
  journal={Systems},
  volume={11},
  number={9},
  pages={442},
  year={2023},
  publisher={MDPI}
}

@article{Veres_21,
  title={How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies},
  author={Vereschak, Oleksandra and Bailly, Gilles and Caramiaux, Baptiste},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW2},
  pages={1--39},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{Lai_21,
  title={Towards a science of human-AI decision making: A survey of empirical studies. CoRR abs/2112.11471 (2021)},
  author={Lai, Vivian and Chen, Chacha and Liao, Q Vera and Smith-Renner, Alison and Tan, Chenhao},
  journal={arXiv preprint arXiv:2112.11471},
  year={2021}
}


@article{Ulfer_24,
  title={Shaping a multidisciplinary understanding of team trust in human-AI teams: a theoretical framework},
  author={Ulfert, Anna-Sophie and Georganta, Eleni and Centeio Jorge, Carolina and Mehrotra, Siddharth and Tielman, Myrthe},
  journal={European Journal of Work and Organizational Psychology},
  volume={33},
  number={2},
  pages={158--171},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{Li_24,
  title={Developing trustworthy artificial intelligence: insights from research on interpersonal, human-automation, and human-AI trust},
  author={Li, Yugang and Wu, Baizhou and Huang, Yuqi and Luan, Shenghua},
  journal={Frontiers in Psychology},
  volume={15},
  pages={1382693},
  year={2024},
  publisher={Frontiers Media SA}
}

@article{Tomse_20,
  title={Rapid trust calibration through interpretable and uncertainty-aware AI},
  author={Tomsett, Richard and Preece, Alun and Braines, Dave and Cerutti, Federico and Chakraborty, Supriyo and Srivastava, Mani and Pearson, Gavin and Kaplan, Lance},
  journal={Patterns},
  volume={1},
  number={4},
  year={2020},
  publisher={Elsevier}
}

@article{Burgg_19,
  title={How cognitive biases influence the data verification of safety indicators: a case study in rail},
  author={Burggraaf, Julia and Groeneweg, Jop and Sillem, Simone and Van Gelder, Pieter},
  journal={Safety},
  volume={5},
  number={4},
  pages={69},
  year={2019},
  publisher={MDPI}
}

@article{Merri_14,
  title={Are well-calibrated users effective users? Associations between calibration of trust and performance on an automation-aided task},
  author={Merritt, Stephanie M and Lee, Deborah and Unnerstall, Jennifer L and Huber, Kelli},
  journal={Human Factors},
  volume={57},
  number={1},
  pages={34--47},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{Berth_21,
  title={Measuring individual differences in cognitive biases: The Cognitive Bias Inventory.},
  author={Berthet, Vincent and Autissier, David and de Gardelle, Vincent},
  year={2021}
}

@inproceedings{Vidul_00,
  title={The relationship between mental workload and situation awareness},
  author={Vidulich, Michael A},
  booktitle={Proceedings of the human factors and ergonomics society annual meeting},
  volume={44},
  number={21},
  pages={3--460},
  year={2000},
  organization={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{Endsl_98,
  title={A comparative analysis of SAGAT and SART for evaluations of situation awareness},
  author={Endsley, Mica R and Selcon, Stephen J and Hardiman, Thomas D and Croft, Darryl G},
  booktitle={Proceedings of the human factors and ergonomics society annual meeting},
  volume={42},
  number={1},
  pages={82--86},
  year={1998},
  organization={Sage Publications Sage CA: Los Angeles, CA}
}

@article{Zeril_18,
  title={Transparency in algorithmic and human decision-making: is there a double standard?},
  author={Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
  journal={Philosophy \& Technology},
  volume={32},
  pages={661--683},
  year={2019},
  publisher={Springer}
}

@article{Bader_19,
  title={Algorithmic decision-making? The user interface and its role for human involvement in decisions supported by artificial intelligence},
  author={Bader, Verena and Kaiser, Stephan},
  journal={Organization},
  volume={26},
  number={5},
  pages={655--672},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{Gao22,
  title={Going beyond xai: A systematic survey for explanation-guided learning},
  author={Gao, Yuyang and Gu, Siyi and Jiang, Junji and Hong, Sungsoo Ray and Yu, Dazhou and Zhao, Liang},
  journal={ACM Computing Surveys},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{Crabb_13,
author = {Crabb, Michael},
title = {Human cognitive measurement as a metric within usability studies},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2479492},
doi = {10.1145/2468356.2479492},
abstract = {There has been a growing interest in the impact that age and online abilities can have on an individual's experience of using the Internet. However, the reliance on these factors has not shown to be entirely conclusive. The current paper develops previous work in this area by using cognitive factors as a method to further analyse user experience. In an experiment, a comparison was drawn between older and younger adults examining Internet experience and multiple cognitive abilities. Overall, the results show that cognitive factors can be used to account for a substantial amount of disorientation felt by users and that these factors can be used to improve the understanding of reasons surrounding web usability. It is also shown that previous Internet experience and confidence differentially effect older and younger adults' feelings of disorientation, with increased confidence resulting in higher disorientation in younger adults but not older adults.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {2677–2682},
numpages = {6},
keywords = {usability, human factors, hci, cognitive psychology},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{He_04,
author = {He, Jun},
title = {Knowledge impacts of user participation: a cognitive perspective},
year = {2004},
isbn = {1581138474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/982372.982374},
booktitle = {Proceedings of the 2004 SIGMIS Conference on Computer Personnel Research: Careers, Culture, and Ethics in a Networked Environment},
pages = {1–7},
numpages = {7},
keywords = {user participation, transactive memory systems, team mental models, team cognition, knowledge process, information systems development},
location = {Tucson, AZ, USA},
series = {SIGMIS CPR '04}
}


@inproceedings{Mentis_09,
author = {Mentis, Helena M. and Bach, Paula M. and Hoffman, Blaine and Rosson, Mary Beth and Carroll, John M.},
title = {Development of decision rationale in complex group decision making},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1341–1350},
numpages = {10},
keywords = {cscw, decision making, rationale, rst},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{Fan_07,
author = {Fan, Xiaocong and Yen, John},
title = {Realistic cognitive load modeling for enhancing shared mental models in human-agent collaboration},
year = {2007},
isbn = {9788190426275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
articleno = {60},
numpages = {8},
keywords = {shared belief maps, multi-party communication, human-centered teamwork, cognitive modeling},
location = {Honolulu, Hawaii},
series = {AAMAS '07}
}

@article{Retzl_24,
author = {Retzlaff, Carl Orge and Das, Srijita and Wayllace, Christabel and Mousavi, Payam and Afshari, Mohammad and Yang, Tianpei and Saranti, Anna and Angerschmid, Alessa and Taylor, Matthew E. and Holzinger, Andreas},
title = {Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities},
year = {2024},
issue_date = {Apr 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {79},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = {jan},
numpages = {57}
}

@inproceedings{Amir_18,
author = {Amir, Ofra and Doshi-Velez, Finale and Sarne, David},
title = {Agent Strategy Summarization},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1203–1207},
numpages = {5},
keywords = {strategy summarization, explainable ai},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{Dubie_22,
author = {Dubiel, Mateusz and Daronnat, Sylvain and Leiva, Luis A.},
title = {Conversational Agents Trust Calibration: A User-Centred Perspective to Design},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {30},
numpages = {6},
keywords = {Conversational Agents, Design ethics, Trust, User-centred Design},
location = {<conf-loc>, <city>Glasgow</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CUI '22}
}


@inproceedings{Sklar_18,
author = {Sklar, Elizabeth I. and Azhar, Mohammad Q.},
title = {Explanation through Argumentation},
year = {2018},
isbn = {9781450359535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 6th International Conference on Human-Agent Interaction},
pages = {277–285},
numpages = {9},
keywords = {computational argumentation, explainable ai, human-robot interaction},
location = {Southampton, United Kingdom},
series = {HAI '18}
}

@inproceedings{Barko_24,
author = {Barkouki, Tammer H. and Chuang, Ian T. and Robinson, Stephen K.},
title = {"What Will You Do Next?" Designing and Evaluating Explanation Generation Using Behavior Trees for Projection-Level XAI},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {223–227},
numpages = {5},
keywords = {behavior trees (BTs), explainable AI (XAI), human-robot interaction (HRI)},
location = {<conf-loc>, <city>Boulder</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {HRI '24}
}
@article{Peter_24,
author = {Peters, Uwe and Carman, Mary},
title = {Cultural Bias in Explainable AI Research: A Systematic Analysis},
year = {2024},
issue_date = {Apr 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {79},
issn = {1076-9757},
month = {mar},
numpages = {30}
}

@inproceedings{Chen_15,
  title={Agent transparency for human-agent teaming effectiveness},
  author={Chen, Jessie YC and Barnes, Michael J},
  booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics},
  pages={1381--1385},
  year={2015},
  organization={IEEE}
}

@article{Chen_14,
  title={Human--agent teaming for multirobot control: A review of human factors issues},
  author={Chen, Jessie YC and Barnes, Michael J},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={44},
  number={1},
  pages={13--29},
  year={2014},
  publisher={IEEE}
}
%%%%%%%%%%%%%% ACM results%%%%%%%%%%%%%%%%%%%%
@InProceedings{Chen_17,
  author    = {Chen, Jessie Y.C. and Selkowitz, Anthony R. and Stowers, Kimberly and Lakhmani, Shan G. and Barnes, Michael J.},
  booktitle = {Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
  title     = {Human-Autonomy Teaming and Agent Transparency},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {91–92},
  publisher = {Association for Computing Machinery},
  series    = {HRI '17},
  comment   = {Autonomous Squad Member (ASM);
SAT model; no stat.significance between increased workload and level of agent transparency},
  isbn      = {9781450348850},
  keywords  = {agent transparency, autonomy, human agent teaming, human robot interaction, interface design, situation awareness},
  location  = {Vienna, Austria},
  numpages  = {2}
}

@InProceedings{Daron_20,
  author    = {Daronnat, Sylvain and Azzopardi, Leif and Halvey, Martin and Dubiel, Mateusz},
  booktitle = {Proceedings of the 8th International Conference on Human-Agent Interaction},
  title     = {Impact of Agent Reliability and Predictability on Trust in Real Time Human-Agent Collaboration},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {131–139},
  publisher = {Association for Computing Machinery},
  series    = {HAI '20},
  isbn      = {9781450380546},
  keywords  = {gaming and serious games, hai experimental methods, human-virtual agent interaction},
  location  = {Virtual Event, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3406499.3415063},
}
@InProceedings{Ehsan_24,
  author    = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O},
  booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
  title     = {The Who in XAI: How AI Background Shapes Perceptions of AI Explanations},
  year      = {2024},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CHI '24},
  articleno = {316},
  isbn      = {9798400703300},
  keywords  = {Explainable AI, Human-Centered Explainable AI, User Characteristics},
  numpages  = {32}
}

@InProceedings{Hanna_15,
  author    = {Hanna, Nader and Richards, Deborah and others},
  booktitle = {COOS@ AAMAS},
  title     = {Do Birds of a Feather Work Better Together? The Impact of Virtual Agent Personality on a Shared Mental Model with Humans during Collaboration.},
  year      = {2015},
  pages     = {28--37},
  comment   = {},
}

@InProceedings{Maehi_23,
  author    = {Maehigashi, Akihiro and Fukuchi, Yosuke and Yamada, Seiji},
  booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
  title     = {Experimental Investigation of Human Acceptance of AI Suggestions with Heatmap and Pointing-based XAI},
  year      = {2023},
  address   = {New York, NY, USA},
  pages     = {291–298},
  publisher = {Association for Computing Machinery},
  series    = {HAI '23},
  comment   = {check because robot; but no direct interaction; 248 participants recruited via yahoo JP; two-factor between-participants design, task: drowsiness detection w/wo heatmap, AI pointing to heatmap, robot pointing to heatmap; study could be used as we have CAM in the XAI section},
  doi       = {10.1145/3623809.3623834},
  isbn      = {9798400708244},
  keywords  = {AI, Heatmap, Pointing, Reliance and compliance, Robot, Saliency map, Trust, XAI},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3623809.3623834},
}


@Article{Schel_22b,
  author     = {Schelble, Beau G. and Flathmann, Christopher and Musick, Geoff and McNeese, Nathan J. and Freeman, Guo},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  title      = {I See You: Examining the Role of Spatial Information in Human-Agent Teams},
  year       = {2022},
  month      = {nov},
  number     = {CSCW2},
  volume     = {6},
  address    = {New York, NY, USA},
  articleno  = {374},
  comment    = {used for review, Schelb_22a is a preceding study},
  doi        = {10.1145/3555099},
  issue_date = {November 2022},
  keywords   = {artificial intelligence, human-AI interaction, human-AI teaming, shared mental model, spatial information, team cognition},
  numpages   = {27},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3555099},
}


@Article{Zhang_23,
  author     = {Zhang, Rui and Duan, Wen and Flathmann, Christopher and McNeese, Nathan and Freeman, Guo and Williams, Alyssa},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  title      = {Investigating AI Teammate Communication Strategies and Their Impact in Human-AI Teams for Effective Teamwork},
  year       = {2023},
  month      = {oct},
  number     = {CSCW2},
  volume     = {7},
  articleno  = {281},
  comment    = {HAT dyadic teams, focus on communication (style) and trust level, 60 participants, online team game; eval: interviews; pre-study: adapted NARS; pro-active AI communication preferred, otherwise seen as "individual" AI, which was seen as not collaborative; limitations: communication beyond dyadic teams (overwhelming information) or less interdependence (more independent) workers in decentralized scenarios.},
  doi        = {10.1145/3610072},
  issue_date = {October 2023},
  keywords   = {communication strategy, human-AI communication, human-AI coordination, human-AI teaming, situation awareness, trust},
  numpages   = {31},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3610072},
}

@InProceedings{Zheng_23,
  author    = {Zheng, Chengbo and Wu, Yuheng and Shi, Chuhan and Ma, Shuai and Luo, Jiehui and Ma, Xiaojuan},
  booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  title     = {Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making},
  year      = {2023},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CHI '23},
  articleno = {351},
  comment   = {essay ranking AI in team of two humans, wizard of oz design; XAI component: Shapley values, counter-factual; evaluation: survey, open questions and Likert scale; eval: plausibility plays a role but also atttitude towards AI; authors advocate socio-technological view as group decision has underlyingh social components (prejudice, stereotypes); possible conformity effects; AI may change group dynamics},
  doi       = {10.1145/3544548.3581131},
  isbn      = {9781450394215},
  keywords  = {automated essay grading, group decision-making, human-AI collaboration, qualitative study},
  numpages  = {19},
  url       = {https://doi.org/10.1145/3544548.3581131},
}

@InProceedings{Guill_23,
  author    = {Le Guillou, Marin and Pr\'{e}vot, Laurent and Berberian, Bruno},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  title     = {Trusting Artificial Agents: Communication Trumps Performance},
  year      = {2023},
  address   = {Richland, SC},
  pages     = {299–306},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '23},
  comment   = {"overcooked" environment; shared mental models; behavioral measures; trust; HAT},
  isbn      = {9781450394321},
  keywords  = {XAI, cognition, user-study},
  numpages  = {8},
}

%%%%%%%%%%%% IEEE results %%%%%%%%%%%%%%%%%%%%%%%%%
@article{Chen_21b,
  title={Developing an improved ACT-R model for pilot situation awareness measurement},
  author={Chen, Hao and Liu, Shuang and Pang, Liping and Wanyan, Xiaoru and Fang, Yufeng},
  journal={IEEE Access},
  volume={9},
  pages={122113--122124},
  year={2021},
  publisher={IEEE}
}

@article{Damar_18,
  title={Common metrics to benchmark human-machine teams (HMT): A review},
  author={Damacharla, Praveen and Javaid, Ahmad Y and Gallimore, Jennie J and Devabhaktuni, Vijay K},
  journal={IEEE Access},
  volume={6},
  pages={38637--38655},
  year={2018},
  publisher={IEEE}
}

@Article{Marte_21,
  author    = {Martelli, Michele and Virdis, Antonio and Gotta, Alberto and Cassar{\`a}, Pietro and Di Summa, Maria},
  journal   = {IEEE Access},
  title     = {An outlook on the future marine traffic management system for autonomous ships},
  year      = {2021},
  pages     = {157316--157328},
  volume    = {9},
  comment   = {for related work},
  publisher = {IEEE},
}

@Article{Melo_23,
  author    = {Melo, Glaucia and Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
  journal   = {IEEE Access},
  title     = {Identifying factors that impact levels of automation in autonomous systems},
  year      = {2023},
  comment   = {for related work},
  publisher = {IEEE},
}

@Article{Metca_21,
  author    = {Metcalfe, Jason S and Perelman, Brandon S and Boothe, David L and Mcdowell, Kaleb},
  journal   = {IEEE Access},
  title     = {Systemic oversimplification limits the potential for human-AI partnership},
  year      = {2021},
  pages     = {70242--70260},
  volume    = {9},
  comment   = {used for related work},
  publisher = {IEEE},
}

@Article{Park_23,
  author    = {Park, Jeonghong and Kang, Minju and Lee, Yeongjun and Jung, Jongdae and Choi, Hyun-Taek and Choi, Jinwoo},
  journal   = {IEEE Access},
  title     = {Multiple Autonomous Surface Vehicles for Autonomous Cooperative Navigation Tasks in a Marine Environment: Development and Preliminary Field Tests},
  year      = {2023},
  pages     = {36203--36217},
  volume    = {11},
  comment   = {perhaps for related work regarding autonomous vessels},
  publisher = {IEEE},
}

@Article{Siebk_22,
  author    = {Siebke, Christian and Mai, Marcus and Prokop, G{\"u}nther},
  journal   = {IEEE transactions on intelligent transportation systems},
  title     = {What do traffic simulations have to provide for virtual road safety assessment? Human error modeling in traffic simulations},
  year      = {2022},
  number    = {2},
  pages     = {1419--1436},
  volume    = {24},
  comment   = {interesting angle of human error and how systems can mitigate them. perhaps useful for related work},
  publisher = {IEEE},
}

@Article{Thomb_20,
  author    = {Thombre, Sarang and Zhao, Zheng and Ramm-Schmidt, Henrik and Garc{\'\i}a, Jos{\'e} M Vallet and Malkam{\"a}ki, Tuomo and Nikolskiy, Sergey and Hammarberg, Toni and Nuortie, Hiski and Bhuiyan, M Zahidul H and S{\"a}rkk{\"a}, Simo and others},
  journal   = {IEEE transactions on intelligent transportation systems},
  title     = {Sensors and AI techniques for situational awareness in autonomous ships: A review},
  year      = {2020},
  number    = {1},
  pages     = {64--83},
  volume    = {23},
  comment   = {review for maritime section},
  publisher = {IEEE},
}

@Article{Yan_21,
  author    = {Yan, Shengyuan and Yao, Kai and Tran, Cong Chi},
  journal   = {Ieee Access},
  title     = {Using artificial neural network for predicting and evaluating situation awareness of operator},
  year      = {2021},
  pages     = {20143--20155},
  volume    = {9},
  comment   = {useful for maritime context but also situational awareness and operator performance},
  publisher = {IEEE},
}

%%%%%%%%%%%%% Science Direct results %%%%%%%%%%%%%%%%%%%%%%
@Article{Haque_23,
  author   = {AKM Bahalul Haque and A.K.M. Najmul Islam and Patrick Mikalef},
  journal  = {Technological Forecasting and Social Change},
  title    = {Explainable Artificial Intelligence (XAI) from a user perspective: A synthesis of prior literature and problematizing avenues for future research},
  year     = {2023},
  issn     = {0040-1625},
  pages    = {122120},
  volume   = {186},
  comment  = {use in review table},
  keywords = {Explainable AI (XAI), XAI effects, Trust, Transparency, Understandability, AI Adoption, AI Use}
}

@Article{Saeed_23,
  author   = {Waddah Saeed and Christian Omlin},
  journal  = {Knowledge-Based Systems},
  title    = {Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities},
  year     = {2023},
  issn     = {0950-7051},
  pages    = {110273},
  volume   = {263},
  comment  = {for review table},
  doi      = {https://doi.org/10.1016/j.knosys.2023.110273},
  keywords = {Explainable AI (XAI), Interpretable AI, Black-box, Machine learning, Deep learning, Meta-survey, Responsible AI}
}

@Article{Mille_19,
  author   = {Tim Miller},
  journal  = {Artificial Intelligence},
  title    = {Explanation in artificial intelligence: Insights from the social sciences},
  year     = {2019},
  issn     = {0004-3702},
  pages    = {1-38},
  volume   = {267},
  comment  = {use for review table},
  doi      = {https://doi.org/10.1016/j.artint.2018.07.007},
  keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
}

@Article{Hou_24,
  author   = {Tsung-Yu Hou and Yu-Chia Tseng and Chien Wen (Tina) Yuan},
  journal  = {International Journal of Information Management},
  title    = {Is this AI sexist? The effects of a biased AI’s anthropomorphic appearance and explainability on users’ bias perceptions and trust},
  year     = {2024},
  issn     = {0268-4012},
  pages    = {102775},
  volume   = {76},
  comment  = {use for study review (maybe a bit far-fetched but perhaps fit in depending on study evaluation structure)},
  keywords = {Human-AI interaction, Explainable AI, Trust, Anthropomorphism, Bias, Gender},
  url      = {https://www.sciencedirect.com/science/article/pii/S0268401224000239},
}


%%%%%%%%%%% studies Google Scholar results

@article{Leich_23,
  title={Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task},
  author={Leichtmann, Benedikt and Humer, Christina and Hinterreiter, Andreas and Streit, Marc and Mara, Martina},
  comment = {study; online mushroom picking game (app)},
  journal={Computers in Human Behavior},
  volume={139},
  pages={107539},
  year={2023},
  publisher={Elsevier}
}



  author    = {Alexander, Veronika and Blinder, Collin and Zak, Paul J},
  journal   = {Computers in Human Behavior},
  title     = {Why trust an algorithm? Performance, cognition, and neurophysiology},
  year      = {2018},
  pages     = {279--288},
  volume    = {89},
  comment   = {selected for study},
  publisher = {Elsevier},
}

@Article{Dietv_15,
  author    = {Dietvorst, Berkeley J and Simmons, Joseph P and Massey, Cade},
  journal   = {Journal of experimental psychology: General},
  title     = {Algorithm aversion: people erroneously avoid algorithms after seeing them err.},
  year      = {2015},
  number    = {1},
  pages     = {114},
  volume    = {144},
  comment   = {selected for study; trust/trust repair; algorithm aversion},
  publisher = {American Psychological Association},
}

@Article{Paras_09,
  author    = {Parasuraman, Raja and Cosenzo, Keryl A and De Visser, Ewart},
  journal   = {Military Psychology},
  title     = {Adaptive automation for human supervision of multiple uninhabited vehicles: Effects on change detection, situation awareness, and mental workload},
  year      = {2009},
  number    = {2},
  pages     = {270--297},
  volume    = {21},
  comment   = {selected for study},
  publisher = {Taylor \& Francis},
}

@Article{Pak_17,
  author    = {Pak, Richard and Rovira, Ericka and McLaughlin, Anne Collins and Baldwin, Natalee},
  journal   = {Theoretical issues in ergonomics science},
  title     = {Does the domain of technology impact user trust? Investigating trust in automation across different consumer-oriented domains in young adults, military, and older adults},
  year      = {2017},
  number    = {3},
  pages     = {199--220},
  volume    = {18},
  comment   = {selected for study review},
  publisher = {Taylor \& Francis},
}

@article{Palej_21,
  title={The utility of explainable ai in ad hoc human-machine teaming},
  author={Paleja, Rohan and Ghuy, Muyleng and Ranawaka Arachchige, Nadun and Jensen, Reed and Gombolay, Matthew},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={610--623},
  year={2021}
}

@Article{Gomez_23,
  author    = {Gomez, Catalina and Unberath, Mathias and Huang, Chien-Ming},
  journal   = {International Journal of Human-Computer Studies},
  title     = {Mitigating knowledge imbalance in AI-advised decision-making through collaborative user involvement},
  year      = {2023},
  pages     = {102977},
  volume    = {172},
  comment   = {selected for study review},
  publisher = {Elsevier},
}


@InProceedings{Alipo_20,
  author    = {Alipour, Kamran and Ray, Arijit and Lin, Xiao and Schulze, Jurgen P. and Yao, Yi and Burachas, Giedrius T.},
  booktitle = {2020 IEEE International Conference on Humanized Computing and Communication with Artificial Intelligence (HCCAI)},
  title     = {The Impact of Explanations on AI Competency Prediction in VQA},
  year      = {2020},
  pages     = {25-32},
  comment   = {study},
  doi       = {10.1109/HCCAI49649.2020.00010},
  keywords  = {Visualization;Correlation;Computational modeling;Bit error rate;Cognitive science;Artificial intelligence;Task analysis;Explainable AI, Visual Question Answering, Interpretability},
}

@InProceedings{Chaze_22,
  author    = {Chazette, Larissa and Klös, Verena and Herzog, Florian and Schneider, Kurt},
  booktitle = {2022 IEEE 30th International Requirements Engineering Conference (RE)},
  title     = {Requirements on Explanations: A Quality Framework for Explainability},
  year      = {2022},
  pages     = {140-152},
  comment   = {study on navigation task (preceded by a lit. review for application of their quality framework)},
  doi       = {10.1109/RE54965.2022.00019},
  keywords  = {Navigation;Aggregates;Software;Requirements engineering;Guidelines;Information systems;Explainability;Explanations;Case Study;NonFunctional Requirements;Quality Aspects;Explainable Systems},
}


@article{Choud_23,
  title={Impact of cognitive workload and situation awareness on clinicians’ willingness to use an artificial intelligence system in clinical practice},
  author={Choudhury, Avishek and Asan, Onur},
  journal={IISE Transactions on Healthcare Systems Engineering},
  volume={13},
  number={2},
  pages={89--100},
  year={2023},
  comment={no open access},
  publisher={Taylor \& Francis}
}

@Article{Confa_24,
  author   = {Confalonieri, Roberto and Alonso-Moral, Jose Maria},
  journal  = {IEEE Intelligent Systems},
  title    = {An Operational Framework for Guiding Human Evaluation in Explainable and Trustworthy Artificial Intelligence},
  year     = {2024},
  issn     = {1941-1294},
  month    = {Jan},
  number   = {1},
  pages    = {18-28},
  volume   = {39},
  abstract = {The assessment of explanations by humans presents a significant challenge within the context of explainable and trustworthy artificial intelligence. This is attributed not only to the absence of universal metrics and standardized evaluation methods but also to the complexities tied to devising user studies that assess the perceived human comprehensibility of these explanations. To address this gap, we introduce a survey-based methodology for guiding the human evaluation of explanations. This approach amalgamates leading practices from existing literature and is implemented as an operational framework. This framework assists researchers throughout the evaluation process, encompassing hypothesis formulation, online user study implementation and deployment, and analysis and interpretation of collected data. The application of this framework is exemplified through two practical user studies.},
  comment  = {study},
  doi      = {10.1109/MIS.2023.3334639},
  keywords = {Artificial intelligence;Taxonomy;Explainable AI;Intelligent systems;Cognitive science;Ethics;Task analysis;Trusted computing;Human factors;Performance evaluation;User centered design;User experience},
}

@article{Efend_20,
  title={Slow response times undermine trust in algorithmic (but not human) predictions},
  author={Efendi{\'c}, Emir and Van de Calseyde, Philippe PFM and Evans, Anthony M},
  journal={Organizational Behavior and Human Decision Processes},
  volume={157},
  pages={103--114},
  year={2020},
  publisher={Elsevier}
}

@Article{Harba_24,
  author    = {Harbarth, Lydia and G{\"o}{\ss}wein, Eva and Bodemer, Daniel and Schnaubert, Lenka},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {(Over) Trusting AI Recommendations: How System and Person Variables Affect Dimensions of Complacency},
  year      = {2024},
  pages     = {1--20},
  comment   = {study; 90 participants; transparent/non~ recommendations in traffic situation},
  publisher = {Taylor \& Francis},
}

@Article{Lee_19,
  author   = {Yeonjoo Lee and Miyeon Ha and Sujeong Kwon and Yealin Shim and Jinwoo Kim},
  journal  = {Computers in Human Behavior},
  title    = {Egoistic and altruistic motivation: How to induce users’ willingness to help for imperfect AI},
  year     = {2019},
  issn     = {0747-5632},
  pages    = {180-196},
  volume   = {101},
  abstract = {Although artificial intelligence is a growing area of research, several problems remain. One such problem of particular importance is the low accuracy of predictions. This paper suggests that users' help is a practical approach to improve accuracy and it considers four factors that trigger users' willingness to help for an imperfect AI system. The two factors covered in Study 1 are utilitarian benefit based on egoistic motivation, and empathy based on altruistic motivation. In Study 2, utilitarian benefit is divided into explainable AI and monetary reward. The results indicate that two variables, namely empathy and monetary reward, have significant positive effects on willingness to help, and monetary reward is the strongest stimulus. In addition, explainable AI is shown to be positively associated with trust in AI. This study applies social studies of help motivation to the HCI field in order to induce users' willingness to help for an imperfect AI. The triggers of help motivation, empathy and monetary reward, can be utilized to induce the users’ voluntary engagement in the loop with an imperfect AI.},
  comment  = {related work, perhaps interesting for trust repair too; focus on imperfect ai interesting},
  doi      = {https://doi.org/10.1016/j.chb.2019.06.009},
  keywords = {AI (Artificial Intelligence), Human in the loop (HITL), Willingness to help, Trust, Explainable AI, Food image recognition},
  url      = {https://www.sciencedirect.com/science/article/pii/S0747563219302274},
}

@Article{Ma_24,
  author         = {Ma, Jun and Feng, Xuejing},
  journal        = {Sustainability},
  title          = {Analysing the Effects of Scenario-Based Explanations on Automated Vehicle HMIs from Objective and Subjective Perspectives},
  year           = {2024},
  issn           = {2071-1050},
  number         = {1},
  volume         = {16},
  abstract       = {Automated vehicles (AVs) are recognized as one of the most effective measures to realize sustainable transport. These vehicles can reduce emissions and environmental pollution, enhance accessibility, improve safety, and produce economic benefits through congestion reduction and cost savings. However, the consumer acceptance of and trust in these vehicles are not ideal, which affects the diffusion speed of AVs on the market. Providing transparent explanations of AV behaviour is a method for building confidence and trust in AV technologies. In this study, we investigated the explainability of user interface information in an Automated Valet Parking (AVP) system—one of the first L4 automated driving systems with a large commercial landing. Specifically, we proposed a scenario-based explanation framework based on explainable AI and examined the effects of these explanations on drivers’ objective and subjective performance. The results of Experiment 1 indicated that the scenario-based explanations effectively improved drivers’ situational trust and user experience (UX), thereby enhancing the perception and understanding that drivers had of the system’s intelligence capabilities. These explanations significantly reduced the mental workload and elevated the user performance in objective evaluations. In Experiment 2, we uncovered distinct explainability preferences among new and frequent users. New users sought increased trust and transparency, benefiting from guided explanations. In contrast, frequent users emphasised efficiency and driving safety. The final experimental results confirmed that solutions customised for different segments of the population are significantly more effective, satisfying, and trustworthy than generic solutions. These findings demonstrate that the explanations for individual differences, based on our proposed scenario-based framework, have significant implications for the adoption and sustainability of AVs.},
  article-number = {63},
  comment        = {study},
  doi            = {10.3390/su16010063},
  url            = {https://www.mdpi.com/2071-1050/16/1/63},
}

@Article{Malli_23,
  author    = {Mallick, Rohit and Flathmann, Christopher and Lancaster, Caitlin and Hauptman, Allyson and McNeese, Nathan and Freeman, Guo},
  journal   = {Behaviour \& Information Technology},
  title     = {The pursuit of happiness: the power and influence of AI teammate emotion in human-AI teamwork},
  year      = {2023},
  pages     = {1--25},
  comment   = {study},
  publisher = {Taylor \& Francis},
}

@Article{Merri_13,
  author    = {Merritt, Stephanie M and Heimbaugh, Heather and LaChapell, Jennifer and Lee, Deborah},
  journal   = {Human factors},
  title     = {I trust it, but I don’t know why: Effects of implicit attitudes toward automation on trust in an automated system},
  year      = {2013},
  number    = {3},
  pages     = {520--534},
  volume    = {55},
  comment   = {study},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{Papen_22,
  author     = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
  journal    = {ACM Trans. Comput.-Hum. Interact.},
  title      = {It’s Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI},
  year       = {2022},
  issn       = {1073-0516},
  month      = {mar},
  number     = {4},
  volume     = {29},
  abstract   = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier’s accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results.},
  address    = {New York, NY, USA},
  articleno  = {35},
  comment    = {study},
  doi        = {10.1145/3495013},
  issue_date = {August 2022},
  keywords   = {explanation fidelity, user trust, minimum explanations, machine learning, Explainable AI},
  numpages   = {33},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3495013},
}

@Article{Schmi_20,
  author    = {Schmidt, Philipp and Biessmann, Felix and Teubner, Timm},
  journal   = {Journal of Decision Systems},
  title     = {Transparency and trust in artificial intelligence systems},
  year      = {2020},
  number    = {4},
  pages     = {260--278},
  volume    = {29},
  comment   = {study (online), text classification},
  publisher = {Taylor \& Francis},
}

@Article{Shuln_24,
  author    = {Shulner-Tal, Avital and Kuflik, Tsvi and Kliger, Doron and Mancini, Azzurra},
  journal   = {International Journal of Human--Computer Interaction},
  title     = {Who Made That Decision and Why? Users’ Perceptions of Human Versus AI Decision-Making and the Power of Explainable-AI},
  year      = {2024},
  pages     = {1--18},
  comment   = {study},
  publisher = {Taylor \& Francis},
}

@InProceedings{Thale_21,
  author    = {Thaler, Anna Magdalena and Schmid, Ute},
  booktitle = {Proceedings of the annual meeting of the cognitive science society},
  title     = {Explaining machine learned relational concepts in visual domains-effects of perceived accuracy on joint performance and trust},
  year      = {2021},
  number    = {43},
  volume    = {43},
  comment   = {study},
}

@Article{Zang_22,
  author    = {Zang, Jing and Jeon, Myounghoon},
  journal   = {Multimodal Technologies and Interaction},
  title     = {The effects of transparency and reliability of in-vehicle intelligent agents on driver perception, takeover performance, workload and situation awareness in conditionally automated vehicles},
  year      = {2022},
  number    = {9},
  pages     = {82},
  volume    = {6},
  comment   = {study; agents in vehicles},
  publisher = {MDPI},
}

@InProceedings{Zhang_20,
  author    = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  title     = {Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {295–305},
  publisher = {Association for Computing Machinery},
  series    = {FAT* '20},
  abstract  = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
  comment   = {study},
  doi       = {10.1145/3351095.3372852},
  isbn      = {9781450369367},
  keywords  = {trust, explainable AI, decision support, confidence},
  location  = {Barcelona, Spain},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3351095.3372852},
}

@article{Wohle_23,
  title={Agent transparency in mixed-initiative multi-UxV control: How should intelligent agent collaborators speak their minds?},
  author={Wohleber, Ryan W and Stowers, Kimberly and Barnes, Michael and Chen, Jessie YC},
  journal={Computers in Human Behavior},
  volume={148},
  pages={107866},
  year={2023},
  publisher={Elsevier}
}



@Comment{jabref-meta: databaseType:bibtex;}
